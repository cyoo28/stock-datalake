{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uB_hhdZs40Xb"
   },
   "source": [
    "# Code Exclusive to Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14327,
     "status": "ok",
     "timestamp": 1736985842772,
     "user": {
      "displayName": "Charles Yoo",
      "userId": "10848566476897055966"
     },
     "user_tz": 300
    },
    "id": "XgNE46dr7JFc",
    "outputId": "edb7b13e-02cf-4e25-fb76-b8c92433bceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "  # Mount Google Drive to notebook\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  import sys\n",
    "  sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbzRoqao8Z87"
   },
   "outputs": [],
   "source": [
    "if 'COLAB_GPU' in os.environ:\n",
    "  # Set configuration file to access AWS\n",
    "  os.environ['AWS_CONFIG_FILE']=\"/content/gdrive/My Drive/cred-stockdata.txt\"\n",
    "  # Set environment variables\n",
    "  os.environ[\"bucket\"] = \"026090555438-stockdata\"\n",
    "  os.environ[\"rssKey\"] = \"rssList.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1736985842772,
     "user": {
      "displayName": "Charles Yoo",
      "userId": "10848566476897055966"
     },
     "user_tz": 300
    },
    "id": "ilmY-YssHd1H",
    "outputId": "aff2d869-48d2-4538-82b8-2cb6cb545581"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n# CNBC Top News\\ntopNews = {\"network\":\"CNBC\", \"feed\":\"Top News\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114\"}\\n# Earnings\\nearning = {\"network\":\"CNBC\", \"feed\":\"Earnings\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839135\"}\\n# Economy\\neconomy = {\"network\":\"CNBC\", \"feed\":\"Economy\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=20910258\"}\\n# Finance\\nfinance = {\"network\":\"CNBC\", \"feed\":\"Finance\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664\"}\\n# Tech\\ntech = {\"network\":\"CNBC\", \"feed\":\"Tech\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910\"}\\n# Investing\\ninvest = {\"network\":\"CNBC\", \"feed\":\"Investing\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069\"}\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# CNBC Top News\n",
    "topNews = {\"network\":\"CNBC\", \"feed\":\"Top News\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114\"}\n",
    "# Earnings\n",
    "earning = {\"network\":\"CNBC\", \"feed\":\"Earnings\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839135\"}\n",
    "# Economy\n",
    "economy = {\"network\":\"CNBC\", \"feed\":\"Economy\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=20910258\"}\n",
    "# Finance\n",
    "finance = {\"network\":\"CNBC\", \"feed\":\"Finance\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664\"}\n",
    "# Tech\n",
    "tech = {\"network\":\"CNBC\", \"feed\":\"Tech\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910\"}\n",
    "# Investing\n",
    "invest = {\"network\":\"CNBC\", \"feed\":\"Investing\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aHDFHqfNEeU"
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N52GAxVRMm6N"
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import boto3\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppHfapzEzUBe"
   },
   "source": [
    "# AccessS3 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-1TvTw1zT5K"
   },
   "outputs": [],
   "source": [
    "# Class for accessing s3\n",
    "class AccessS3:\n",
    "  def __init__(self):\n",
    "    # Setup s3 client\n",
    "    session = boto3.Session()\n",
    "    self.s3 = session.client('s3')\n",
    "    self.paginator = self.s3.get_paginator('list_objects_v2')\n",
    "\n",
    "  # Get an object\n",
    "  # Arg: bucket [str] **bucket name**,\n",
    "  #      key [str] **object key**,\n",
    "  # Returns: [s3 obj]\n",
    "  def getObj(self, bucket, key):\n",
    "    return self.s3.get_object(Bucket=bucket, Key=key)\n",
    "\n",
    "  # Delete an object\n",
    "  # Arg: bucket [str] **bucket name**,\n",
    "  #      key [str] **object key**\n",
    "  def deleteObj(self, bucket, key):\n",
    "    self.s3.delete_object(Bucket=bucket, Key=key)\n",
    "    print(\"Deleted object at {}\".format(key))\n",
    "    return 0\n",
    "\n",
    "  # Save an object\n",
    "  # Arg: data [obj] **data to be saved**\n",
    "  #      bucket [str] **bucket name**,\n",
    "  #      key [str] **object key**\n",
    "  def saveObj(self, data, bucket, key):\n",
    "    self.s3.put_object(\n",
    "      Body=data,\n",
    "      Bucket=bucket,\n",
    "      Key=key\n",
    "    )\n",
    "    print(\"Saved object at {}\".format(key))\n",
    "    return 0\n",
    "\n",
    "  # Return objects contained in a key\n",
    "  # Arg: bucket [str] **bucket name**,\n",
    "  #      key [str] **object key**\n",
    "  #      sort [str] default None **sort entries by upload date \"newFirst\" newest to oldest \"newLast\" oldest to newest**\n",
    "  # Returns: objs [list of s3 objs] **objects in key**\n",
    "  def scanObjs(self, bucket, key, sort=None):\n",
    "    objs = []\n",
    "    # Get all objects in bucket and key pair\n",
    "    pages = self.paginator.paginate(Bucket=bucket, Prefix=key)\n",
    "    # if you would like to sort the objects by upload dates,\n",
    "    if sort:\n",
    "      for page in pages:\n",
    "        for content in page['Contents']:\n",
    "          # Only add objects that do not end in \"/\" (folder keys end in \"/\")\n",
    "          if not content['Key'].endswith(\"/\"):\n",
    "            objs.append(content)\n",
    "      # Sort by last modified date\n",
    "      lastModified = lambda obj: int(obj['LastModified'].strftime('%s'))\n",
    "      if sort==\"newFirst\":\n",
    "        sortedObjs =  [obj['Key'] for obj in sorted(objs, key=lastModified, reverse=True)]\n",
    "      elif sort==\"newLast\":\n",
    "        sortedObjs =  [obj['Key'] for obj in sorted(objs, key=lastModified)]\n",
    "      return sortedObjs\n",
    "    # otherwise,\n",
    "    else:\n",
    "      # Convert iterator to list\n",
    "      for page in pages:\n",
    "        for content in page['Contents']:\n",
    "          # Only add object keys that do not end in \"/\" (folder keys end in \"/\")\n",
    "          if not content['Key'].endswith(\"/\"):\n",
    "            objs.append(content['Key'])\n",
    "      return objs\n",
    "\n",
    "  # Look up a specific object\n",
    "  # Arg: bucket [str] **bucket name**\n",
    "  #      key [str] **lookup key**\n",
    "  #      subKey [str] **substring of key to look for**\n",
    "  # Returns: matchObjs [list of s3 objs] **object key if it exists**\n",
    "  def lookupObj(self, bucket, key, subKey):\n",
    "    matchObjs = []\n",
    "    # Return objects contained in a key\n",
    "    objs = self.scanObjs(bucket, key)\n",
    "    # Add object keys that contain the lookup substring\n",
    "    for obj in objs:\n",
    "      if subKey in obj:\n",
    "        matchObjs.append(obj)\n",
    "    return matchObjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7RoyHvAzUh2"
   },
   "source": [
    "# SaveStockData Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQI-ts6TzURt"
   },
   "outputs": [],
   "source": [
    "# Class for saving stock data to s3\n",
    "class SaveStockData:\n",
    "  def __init__(self):\n",
    "    session = boto3.Session()\n",
    "    self.s3 = session.client('s3')\n",
    "\n",
    "  # Create key path for metadata\n",
    "  # Arg: newEntry [dict] **entry from RSS feed**,\n",
    "  #      network [str] **network that published the RSS feed**\n",
    "  # Returns: metaKey [str] **meta key**\n",
    "  def createMetaPath(self, newEntry, network):\n",
    "    # Gather necessary info from entry to create key\n",
    "    id = newEntry[\"id\"]\n",
    "    year = newEntry[\"published_parsed\"][0]\n",
    "    month = newEntry[\"published_parsed\"][1]\n",
    "    day = newEntry[\"published_parsed\"][2]\n",
    "    # Create key\n",
    "    metaKey = \"metadata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
    "    return metaKey\n",
    "\n",
    "  # Create key path for textdata\n",
    "  # Arg: newEntry [dict] **entry from RSS feed**,\n",
    "  #      network [str] **network that published the RSS feed**\n",
    "  # Returns: textKey [str] **text key**\n",
    "  def createTextPath(self, newEntry, network):\n",
    "    # Gather necessary info from entry to create key\n",
    "    id = newEntry[\"id\"]\n",
    "    year = newEntry[\"published_parsed\"][0]\n",
    "    month = newEntry[\"published_parsed\"][1]\n",
    "    day = newEntry[\"published_parsed\"][2]\n",
    "    # Create key\n",
    "    textKey = \"textdata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
    "    return textKey\n",
    "\n",
    "  # Collect metadata to save\n",
    "  # Arg: newEntry [dict] **entry from RSS feed**,\n",
    "  #      feed [str] **RSS feed name**\n",
    "  # Returns: metaData [json dict] **meta data**\n",
    "  def collectMetaData(self, newEntry, feed):\n",
    "    title = newEntry[\"title\"]\n",
    "    link = newEntry[\"link\"]\n",
    "    date = newEntry[\"published\"]\n",
    "    sponsor = newEntry[\"metadata_sponsored\"]\n",
    "    metaType = newEntry[\"metadata_type\"]\n",
    "    metaData = json.dumps({\"title\":title,\"link\":link,\"date\":date,\"feed\":feed,\"sponsor\":sponsor,\"type\":metaType})\n",
    "    return metaData\n",
    "\n",
    "  # Collect textdata to save\n",
    "  # Arg: newEntry [dict] **entry from RSS feed**,\n",
    "  #      feed [str] **RSS feed name**\n",
    "  # Returns: textData [json str] **text data**\n",
    "  def collectTextData(self, newEntry):\n",
    "    textData = json.dumps(newEntry[\"text\"])\n",
    "    return textData\n",
    "\n",
    "  # Save data to s3\n",
    "  # Arg: data [obj] **data to be saved**,\n",
    "  #      bucket [str] **bucket name**,\n",
    "  #      key [str] **key to save to**\n",
    "  def saveData(self, data, bucket, key):\n",
    "    self.s3.put_object(\n",
    "        Body=data,\n",
    "        Bucket=bucket,\n",
    "        Key=key\n",
    "    )\n",
    "    print(\"Saved object at {}\".format(key))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwjSigWoMx_w"
   },
   "source": [
    "# Scrape RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSvIKGOu43Cc"
   },
   "outputs": [],
   "source": [
    "# Fetch the entries from an RSS feed\n",
    "# Arg: source [str] **contains RSS feed link**\n",
    "# Returns: entries [list of dict] **contains entries in RSS feed**\n",
    "def fetchRSS(source):\n",
    "  # fetch the RSS feed\n",
    "  RSS = feedparser.parse(source)\n",
    "  # extract the entries from the retrieved feed\n",
    "  entries = RSS.entries\n",
    "  return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezJS6W4UNLK2"
   },
   "source": [
    "# Parse HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktyhYTtANJhh"
   },
   "outputs": [],
   "source": [
    "# Scrape an entry using the url\n",
    "# Arg: url [str] **contains url for the entry**\n",
    "# Returns: soup [BeautifulSoup obj]\n",
    "def scraper(url):\n",
    "  # scrape the data from the url using requests and BeautifulSoup\n",
    "  page = requests.get(url)\n",
    "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "  return soup\n",
    "\n",
    "# Extract text from the scraped/parsed entry\n",
    "# Arg: soup [BeautifulSoup obj]\n",
    "# Returns: result [str] **contains the text of the entry**\n",
    "def textExtract(soup):\n",
    "  # extract the data corresponding to the entry from the soup\n",
    "  entryGroups = soup.find_all(\"div\", {\"class\": \"group\"})\n",
    "  sections = []\n",
    "  for group in entryGroups:\n",
    "    section = group.find_all(\"p\")\n",
    "    if section:\n",
    "      sections.append(section)\n",
    "  # extract the text of the entry\n",
    "  text = []\n",
    "  for section in sections:\n",
    "    for para in section:\n",
    "      text.append(para.get_text())\n",
    "  # return the entry joined as a single string\n",
    "  return (\"\\n\").join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jn0wyZQGrT2I"
   },
   "source": [
    "# Get publication dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aps_rImQrTqK"
   },
   "outputs": [],
   "source": [
    "# Get publication dates of files\n",
    "# Arg: bucket [str] **bucket name**,\n",
    "#      keys [list of str] **list of keys that require dates**\n",
    "#      s3Helper [AccessS3 inst]\n",
    "# Returns: dates [list of str] **list of dates for keys**\n",
    "def getDates(bucket, keys, s3Helper):\n",
    "  dates = []\n",
    "  for key in keys:\n",
    "    date = (json.loads(s3Helper.getObj(bucket, key)['Body'].read().decode()))\n",
    "    dates.append(date[\"date\"])\n",
    "  return dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ei3HbWBPlZi"
   },
   "source": [
    "# Convert publication dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrBZJVB8PlMV"
   },
   "outputs": [],
   "source": [
    "# Convert publication dates of files\n",
    "# Arg: unstrDates [list of str] **unstructed dates**\n",
    "# Returns: [list of str] **list of structured dates**\n",
    "def convertDates(unstrDates):\n",
    "  dateFormat = \"%a, %d %b %Y %H:%M:%S %Z\"\n",
    "  return [datetime.datetime.strptime(unstrDate, dateFormat) for unstrDate in unstrDates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUxoCxQb6OJG"
   },
   "source": [
    "# Save New Entries to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tt1okDhwIGA"
   },
   "outputs": [],
   "source": [
    "# Save the text data and metadata for a new entry\n",
    "# Arg: newEntry [dict] **new entry to be saved**,\n",
    "#      network [str] **network that published the RSS feed**,\n",
    "#      feed [str] **name of the RSS feed**,\n",
    "#      bucket [str] **S3 bucket where data is stored**\n",
    "def saveSingleEntry(newEntry, network, feed, bucket):\n",
    "  stockSaver = SaveStockData()\n",
    "\n",
    "  # scrape and add the text to the entry\n",
    "  entryURL = newEntry[\"link\"]\n",
    "  soup = scraper(entryURL)\n",
    "  result = textExtract(soup)\n",
    "  newEntry[\"text\"] = result\n",
    "\n",
    "  # create object path\n",
    "  metaKey = stockSaver.createMetaPath(newEntry, network)\n",
    "  textKey = stockSaver.createTextPath(newEntry, network)\n",
    "\n",
    "  # collect metadata and text data\n",
    "  metaData = stockSaver.collectMetaData(newEntry, feed)\n",
    "  textData = stockSaver.collectTextData(newEntry)\n",
    "\n",
    "  # upload metadata\n",
    "  stockSaver.saveData(metaData, bucket, metaKey)\n",
    "  # upload text data\n",
    "  stockSaver.saveData(textData, bucket, textKey)\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6_QRSNJLmok"
   },
   "outputs": [],
   "source": [
    "# Save new entries to S3 and update old ones\n",
    "# Arg: entries [list of dict] **entries gathered from feed**\n",
    "#      network [str] **network that published the RSS feed**\n",
    "#      feed [str] **name of the RSS feed**\n",
    "#      bucket [str] **bucket where data is stored**\n",
    "#      s3Helper [AccessS3 inst]\n",
    "# Returns: count [int] **number of entries saved for feed**\n",
    "def saveNewEntries(entries, network, feed, bucket, s3Helper):\n",
    "  # Setup save counter\n",
    "  count = 0\n",
    "  print(\"This feed has {} entries\".format(len(entries)))\n",
    "  # Get the existing entries from s3\n",
    "  existKeys = s3Helper.scanObjs(bucket, \"metadata\")\n",
    "  for entry in entries:\n",
    "    # Get the entry id\n",
    "    entryID = entry[\"id\"]\n",
    "    # if the entry id does not already exist,\n",
    "    if not any(entryID in existKey for existKey in existKeys):\n",
    "      # then save it to s3\n",
    "      print(\"Saving new entry id: {}\".format(entryID))\n",
    "      saveSingleEntry(entry, network, feed, bucket)\n",
    "      count += 1\n",
    "    # otherwise it already exists\n",
    "    else:\n",
    "      # Search for matching entries\n",
    "      matchKeys = [existKey.split(\"/\", 1)[1] for existKey in existKeys if entryID in existKey]\n",
    "      metaKeys = [\"metadata/\"+matchKey for matchKey in matchKeys]\n",
    "      textKeys = [\"textdata/\"+matchKey for matchKey in matchKeys]\n",
    "      # Retrieve and convert dates to comparable datetime objects\n",
    "      matchDates = getDates(bucket, metaKeys, s3Helper)\n",
    "      existDates = convertDates(matchDates)\n",
    "      latestDate = max(existDates)\n",
    "      entryDate = convertDates([entry[\"published\"]])[0]\n",
    "      # if the entry has been updated,\n",
    "      if entryDate > latestDate:\n",
    "        print(\"Updating entry id: {}\".format(entryID))\n",
    "        for metaKey, textKey in zip(metaKeys, textKeys):\n",
    "          # delete the old one(s)\n",
    "          #print(\"Deleted {}\".format(metaKey))\n",
    "          #print(\"Deleted {}\".format(textKey))\n",
    "          s3Helper.deleteObj(bucket, metaKey)\n",
    "          s3Helper.deleteObj(bucket, textKey)\n",
    "        # and save the new one\n",
    "        saveSingleEntry(entry, network, feed, bucket)\n",
    "        count += 1\n",
    "      # otherwise check for duplicates\n",
    "      elif len(matchKeys) > 1:\n",
    "        print(\"Deleting duplicate entries id: {}\".format(entryID))\n",
    "        for metaKey, textKey, existDate in zip(metaKeys, textKeys, existDates):\n",
    "          if not existDate==latestDate:\n",
    "            # and delete the older ones\n",
    "            #print(\"Deleted {}\".format(metaKey))\n",
    "            #print(\"Deleted {}\".format(textKey))\n",
    "            s3Helper.deleteObj(bucket, metaKey)\n",
    "            s3Helper.deleteObj(bucket, textKey)\n",
    "  # Report the total number of entries that have been saved\n",
    "  if count==0:\n",
    "    print('No new entries for {}'.format(feed))\n",
    "  else:\n",
    "    print(\"{} new entries for {}\".format(count, feed))\n",
    "  return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2xBf8an5Pvu"
   },
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnDjLKc35Qa4"
   },
   "outputs": [],
   "source": [
    "def main(event, context):\n",
    "  # Set up variables and AccessS3 class\n",
    "  count = 0\n",
    "  bucket = os.environ[\"bucket\"]\n",
    "  rssKey = os.environ[\"rssKey\"]\n",
    "  s3Helper = AccessS3()\n",
    "  # Retrieve list of RSS feeds to gather entries from\n",
    "  obj = s3Helper.getObj(bucket, rssKey)\n",
    "  rssList = json.loads(obj[\"Body\"].read().decode())\n",
    "  # Gather entries from the feeds and save new ones to S3\n",
    "  count = 0\n",
    "  for rss in rssList:\n",
    "    rssNetwork = rss[\"network\"]\n",
    "    rssFeed = rss[\"feed\"]\n",
    "    rssURL = rss[\"url\"]\n",
    "    # Fetch entries from the current RSS feed\n",
    "    print(\"Currently scraping from {}\".format(rssFeed))\n",
    "    entries = fetchRSS(rssURL)\n",
    "    # Save new entries to S3\n",
    "    count += saveNewEntries(entries, rssNetwork, rssFeed, bucket, s3Helper)\n",
    "  print('{} total entries have been saved'.format(count))\n",
    "  return {\n",
    "      'statusCode': 200\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjSuiHoVI30z"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24784,
     "status": "ok",
     "timestamp": 1736986499533,
     "user": {
      "displayName": "Charles Yoo",
      "userId": "10848566476897055966"
     },
     "user_tz": 300
    },
    "id": "B6Oha02K5hjt",
    "outputId": "3ab8c666-def2-4c78-e694-73798f27fbc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently scraping from Top News\n",
      "This feed has 30 entries\n",
      "No new entries for Top News\n",
      "Currently scraping from Earnings\n",
      "This feed has 30 entries\n",
      "No new entries for Earnings\n",
      "Currently scraping from Economy\n",
      "This feed has 30 entries\n",
      "No new entries for Economy\n",
      "Currently scraping from Finance\n",
      "This feed has 30 entries\n",
      "No new entries for Finance\n",
      "Currently scraping from Tech\n",
      "This feed has 30 entries\n",
      "No new entries for Tech\n",
      "Currently scraping from Investing\n",
      "This feed has 30 entries\n",
      "No new entries for Investing\n",
      "0 total entries have been saved\n",
      "{'statusCode': 200}\n"
     ]
    }
   ],
   "source": [
    "if 'COLAB_GPU' in os.environ:\n",
    "  # import text and test\n",
    "  result = main(None, None)\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-CTageP_w18"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(None, None)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNm0/bOW3dfag8zvslb/039",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
