{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOli+xlylfJur38mVEqD6Qc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Code Exclusive to Colab"],"metadata":{"id":"9ylabT8SkPtI"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5HpOC3G9kFIJ","executionInfo":{"status":"ok","timestamp":1736985867257,"user_tz":300,"elapsed":15586,"user":{"displayName":"Charles Yoo","userId":"10848566476897055966"}},"outputId":"22183a11-6040-460b-f5d6-4bad701eab74"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["import os\n","if 'COLAB_GPU' in os.environ:\n","  # Mount Google Drive to notebook\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","  import sys\n","  sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"]},{"cell_type":"code","source":["if 'COLAB_GPU' in os.environ:\n","  # Set configuration file to access AWS\n","  os.environ['AWS_CONFIG_FILE']=\"/content/gdrive/My Drive/cred-stockdata.txt\"\n","  # Set environment variables\n","  os.environ[\"bucket\"] = \"026090555438-stockdata\""],"metadata":{"id":"COaKf2MVkOdl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import Packages"],"metadata":{"id":"xsO7xm4pkSVo"}},{"cell_type":"code","source":["import json\n","import boto3\n","import datetime\n","from collections import Counter"],"metadata":{"id":"-gsxNliHkS9V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# AccessS3 Class"],"metadata":{"id":"N0jHHz0zzoNz"}},{"cell_type":"code","source":["# Class for accessing s3\n","class AccessS3:\n","  def __init__(self):\n","    # Setup s3 client\n","    session = boto3.Session()\n","    self.s3 = session.client('s3')\n","    self.paginator = self.s3.get_paginator('list_objects_v2')\n","\n","  # Get an object\n","  # Arg: bucket [str] **bucket name**,\n","  #      key [str] **object key**,\n","  # Returns: [s3 obj]\n","  def getObj(self, bucket, key):\n","    return self.s3.get_object(Bucket=bucket, Key=key)\n","\n","  # Delete an object\n","  # Arg: bucket [str] **bucket name**,\n","  #      key [str] **object key**\n","  def deleteObj(self, bucket, key):\n","    self.s3.delete_object(Bucket=bucket, Key=key)\n","    print(\"Deleted object at {}\".format(key))\n","    return 0\n","\n","  # Save an object\n","  # Arg: data [obj] **data to be saved**\n","  #      bucket [str] **bucket name**,\n","  #      key [str] **object key**\n","  def saveObj(self, data, bucket, key):\n","    self.s3.put_object(\n","      Body=data,\n","      Bucket=bucket,\n","      Key=key\n","    )\n","    print(\"Saved object at {}\".format(key))\n","    return 0\n","\n","  # Return objects contained in a key\n","  # Arg: bucket [str] **bucket name**,\n","  #      key [str] **object key**\n","  #      sort [str] default None **sort entries by upload date \"newFirst\" newest to oldest \"newLast\" oldest to newest**\n","  # Returns: objs [list of s3 objs] **objects in key**\n","  def scanObjs(self, bucket, key, sort=None):\n","    objs = []\n","    # Get all objects in bucket and key pair\n","    pages = self.paginator.paginate(Bucket=bucket, Prefix=key)\n","    # if you would like to sort the objects by upload dates,\n","    if sort:\n","      for page in pages:\n","        for content in page['Contents']:\n","          # Only add objects that do not end in \"/\" (folder keys end in \"/\")\n","          if not content['Key'].endswith(\"/\"):\n","            objs.append(content)\n","      # Sort by last modified date\n","      lastModified = lambda obj: int(obj['LastModified'].strftime('%s'))\n","      if sort==\"newFirst\":\n","        sortedObjs =  [obj['Key'] for obj in sorted(objs, key=lastModified, reverse=True)]\n","      elif sort==\"newLast\":\n","        sortedObjs =  [obj['Key'] for obj in sorted(objs, key=lastModified)]\n","      return sortedObjs\n","    # otherwise,\n","    else:\n","      # Convert iterator to list\n","      for page in pages:\n","        for content in page['Contents']:\n","          # Only add object keys that do not end in \"/\" (folder keys end in \"/\")\n","          if not content['Key'].endswith(\"/\"):\n","            objs.append(content['Key'])\n","      return objs\n","\n","  # Look up a specific object\n","  # Arg: bucket [str] **bucket name**\n","  #      key [str] **lookup key**\n","  #      subKey [str] **substring of key to look for**\n","  # Returns: matchObjs [list of s3 objs] **object key if it exists**\n","  def lookupObj(self, bucket, key, subKey):\n","    matchObjs = []\n","    # Return objects contained in a key\n","    objs = self.scanObjs(bucket, key)\n","    # Add object keys that contain the lookup substring\n","    for obj in objs:\n","      if subKey in obj:\n","        matchObjs.append(obj)\n","    return matchObjs"],"metadata":{"id":"h-FW5YbOzoGW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get publication dates"],"metadata":{"id":"iQe3xdf2RbxL"}},{"cell_type":"code","source":["# Get publication dates of files\n","# Arg: bucket [str] **bucket name**,\n","#      keys [list of str] **list of keys that require dates**\n","#      s3Helper [AccessS3 Instance]\n","# Returns: dates [list of str] **list of dates for keys**\n","def getDates(bucket, keys, s3Helper):\n","  dates = []\n","  for key in keys:\n","    date = (json.loads(s3Helper.getObj(bucket, key)['Body'].read().decode()))\n","    dates.append(date[\"date\"])\n","  return dates"],"metadata":{"id":"Ro4BSa4XRbpa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Convert publication dates"],"metadata":{"id":"bytx1QtDQqdY"}},{"cell_type":"code","source":["# Convert publication dates of files\n","# Arg: unstrDates [list of str] **unstructed dates**\n","# Returns: [list of str] **list of structured dates**\n","def convertDates(unstrDates):\n","  dateFormat = \"%a, %d %b %Y %H:%M:%S %Z\"\n","  return [datetime.datetime.strptime(unstrDate, dateFormat) for unstrDate in unstrDates]"],"metadata":{"id":"Zd-p_-DfQqVK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Lookup a key in a list of keys (not accessing s3)"],"metadata":{"id":"z4-4UChfjaWR"}},{"cell_type":"code","source":["# Look up a key in a list of keys\n","# Arg: keys [list of str] **list of keys to look in**\n","#      lookupKey [str] **key to lookup**\n","# Returns: matchKeys [list of str] **list of matching keys**\n","def lookupKeys(keys, lookupKey):\n","  matchKeys = [key for key in keys if lookupKey in key]\n","  return matchKeys"],"metadata":{"id":"r6H2oeTajaj8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Delete Duplicates"],"metadata":{"id":"8cW_cpYHruVF"}},{"cell_type":"code","source":["# Find duplicates and delete old ones\n","# Arg: bucket [str] **bucket name**\n","def deleteDup(bucket):\n","  # Set up AccessS3 class and deletion counter\n","  s3Helper = AccessS3()\n","  count = 0\n","  # Get all object keys and entry ids in the bucket\n","  objs = s3Helper.scanObjs(bucket, \"metadata\")\n","  ids = [obj.rsplit('/', 1)[-1].split('.')[0] for obj in objs]\n","  # Count occurences of each id in the bucket\n","  occurences = Counter(ids)\n","  for id, occurence in occurences.items():\n","    # if the id occurs more than once,\n","    if occurence > 1:\n","      # Get all keys that match the id\n","      matchMetaKeys = lookupKeys(objs, id)\n","      matchTextKeys = [\"textdata/\"+key.split(\"/\",1)[1] for key in matchMetaKeys]\n","      # Get the dates for each occurence and convert them to comparable datetime objects\n","      metaDates = getDates(bucket, matchMetaKeys, s3Helper)\n","      matchDates = convertDates(metaDates)\n","      newDate = max(matchDates)\n","      for metakey, textkey, date in zip(matchMetaKeys, matchTextKeys, matchDates):\n","        # Delete occurences that are not the most recent one\n","        if not date==newDate:\n","          #print(\"Deleted {}\".format(metakey))\n","          #print(\"Deleted {}\".format(textkey))\n","          s3Helper.deleteObj(bucket, metakey)\n","          s3Helper.deleteObj(bucket, textkey)\n","          count += 1\n","  print(\"{} have been deleted\".format(count))\n","  return 0"],"metadata":{"id":"QMkV7kshka1n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# main"],"metadata":{"id":"4x4pWqAUkdvR"}},{"cell_type":"code","source":["def main(event, context):\n","  bucket = os.environ[\"bucket\"]\n","  deleteDup(bucket)\n","  return {\n","      'statusCode': 200\n","  }"],"metadata":{"id":"IEGKgnxkke_W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test"],"metadata":{"id":"Zv99FoGVkkGQ"}},{"cell_type":"code","source":["if 'COLAB_GPU' in os.environ:\n","  result = main(None, None)\n","  print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"qzFl2VBBkmZI","executionInfo":{"status":"ok","timestamp":1736985954757,"user_tz":300,"elapsed":57269,"user":{"displayName":"Charles Yoo","userId":"10848566476897055966"}},"outputId":"c8a09950-3c95-482c-d404-75dafc3f63af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 have been deleted\n","{'statusCode': 200}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ycLGmWzlUTIH"},"execution_count":null,"outputs":[]}]}