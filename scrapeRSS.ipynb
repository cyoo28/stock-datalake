{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhIvZVZgo0SbQsa7DFP1QQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iyoo2018/findatalake/blob/master/scrapeRSS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Exclusive to Colab"
      ],
      "metadata": {
        "id": "uB_hhdZs40Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  import sys\n",
        "  sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"
      ],
      "metadata": {
        "id": "XgNE46dr7JFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb4933a-a940-4019-91c7-97f433b932a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "  os.environ['AWS_CONFIG_FILE']=\"/content/gdrive/My Drive/cred-stockdata.txt\"\n",
        "  os.environ[\"bucket\"] = \"026090555438-stockdata\"\n",
        "  os.environ[\"rssKey\"] = \"rssList.json\""
      ],
      "metadata": {
        "id": "VbzRoqao8Z87"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# CNBC Top News\n",
        "topNews = {\"network\":\"CNBC\", \"feed\":\"Top News\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114\"}\n",
        "# Earnings\n",
        "earning = {\"network\":\"CNBC\", \"feed\":\"Earnings\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839135\"}\n",
        "# Economy\n",
        "economy = {\"network\":\"CNBC\", \"feed\":\"Economy\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=20910258\"}\n",
        "# Finance\n",
        "finance = {\"network\":\"CNBC\", \"feed\":\"Finance\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664\"}\n",
        "# Tech\n",
        "tech = {\"network\":\"CNBC\", \"feed\":\"Tech\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910\"}\n",
        "# Investing\n",
        "invest = {\"network\":\"CNBC\", \"feed\":\"Investing\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069\"}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ilmY-YssHd1H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "13c5e683-bad8-44f9-9bca-414ccf02235d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# CNBC Top News\\ntopNews = {\"network\":\"CNBC\", \"feed\":\"Top News\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114\"}\\n# Earnings\\nearning = {\"network\":\"CNBC\", \"feed\":\"Earnings\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839135\"}\\n# Economy\\neconomy = {\"network\":\"CNBC\", \"feed\":\"Economy\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=20910258\"}\\n# Finance\\nfinance = {\"network\":\"CNBC\", \"feed\":\"Finance\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664\"}\\n# Tech\\ntech = {\"network\":\"CNBC\", \"feed\":\"Tech\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910\"}\\n# Investing\\ninvest = {\"network\":\"CNBC\", \"feed\":\"Investing\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069\"}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ],
      "metadata": {
        "id": "-aHDFHqfNEeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "import boto3\n",
        "import datetime"
      ],
      "metadata": {
        "id": "N52GAxVRMm6N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape RSS Feeds"
      ],
      "metadata": {
        "id": "OwjSigWoMx_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the articles from an RSS feed\n",
        "# Arg: source [string] *contains RSS feed link*\n",
        "# Returns: entries [list of dictionaries] *contains articles in RSS feed*\n",
        "def fetchRSS(source):\n",
        "  # fetch the RSS feed\n",
        "  RSS = feedparser.parse(source)\n",
        "  # extract the entries from the retrieved feed\n",
        "  entries = RSS.entries\n",
        "  return entries"
      ],
      "metadata": {
        "id": "OSvIKGOu43Cc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse HTML"
      ],
      "metadata": {
        "id": "ezJS6W4UNLK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape an article using the url\n",
        "# Arg: url [string] **contains url for the article**\n",
        "# Returns: soup [BeautifulSoup object]\n",
        "def scraper(url):\n",
        "  # scrape the data from the url using requests and BeautifulSoup\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "  return soup\n",
        "\n",
        "# Extract text from the scraped/parsed article\n",
        "# Arg: soup [BeautifulSoup object]\n",
        "# Returns: result [string] **contains the text of the article**\n",
        "def textExtract(soup):\n",
        "  # extract the data corresponding to the article from the soup\n",
        "  articleGroups = soup.find_all(\"div\", {\"class\": \"group\"})\n",
        "  sections = []\n",
        "  for group in articleGroups:\n",
        "    section = group.find_all(\"p\")\n",
        "    if section:\n",
        "      sections.append(section)\n",
        "  # extract the text of the article\n",
        "  text = []\n",
        "  for section in sections:\n",
        "    for para in section:\n",
        "      text.append(para.get_text())\n",
        "  # return the article joined as a single string\n",
        "  return (\"\\n\").join(text)"
      ],
      "metadata": {
        "id": "ktyhYTtANJhh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert publication dates"
      ],
      "metadata": {
        "id": "6Ei3HbWBPlZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert publication dates of files\n",
        "# Arg: unstrDates **unstructed dates** [list of str]\n",
        "# Returns: list of structured dates [list of str]\n",
        "def convertDates(unstrDates):\n",
        "  dateFormat = \"%a, %d %b %Y %H:%M:%S %Z\"\n",
        "  strDates = []\n",
        "  for unstrDate in unstrDates:\n",
        "    strDates.append(datetime.datetime.strptime(unstrDate, dateFormat))\n",
        "  return strDates"
      ],
      "metadata": {
        "id": "SrBZJVB8PlMV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AccessS3 Class"
      ],
      "metadata": {
        "id": "iAXvRUXENIzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AccessS3:\n",
        "  def __init__(self):\n",
        "    session = boto3.Session()\n",
        "    self.s3 = session.client('s3')\n",
        "    self.paginator = self.s3.get_paginator('list_objects_v2')\n",
        "\n",
        "  # Get an object\n",
        "  # Arg: bucket **bucket name** [str],\n",
        "  #      key **object key** [str],\n",
        "  # Returns: object\n",
        "  def getObj(self, bucket, key):\n",
        "    return self.s3.get_object(Bucket=bucket, Key=key)\n",
        "\n",
        "  # Delete an object\n",
        "  # Arg: bucket **bucket name** [str],\n",
        "  #      key **object key** [str]\n",
        "  def deleteObj(self, bucket, key):\n",
        "    self.s3.delete_object(Bucket=bucket, Key=key)\n",
        "    print(\"Deleted object at {}\".format(key))\n",
        "    return 0\n",
        "\n",
        "  # Save an object\n",
        "  # Arg: data **data to be saved**\n",
        "  #      bucket **bucket name** [str],\n",
        "  #      key **object key** [str]\n",
        "  def saveObj(self, data, bucket, key):\n",
        "    self.s3.put_object(\n",
        "      Body=data,\n",
        "      Bucket=bucket,\n",
        "      Key=key\n",
        "    )\n",
        "    print(\"Saved object at {}\".format(key))\n",
        "    return 0\n",
        "\n",
        "  # Look at objects contained in a key\n",
        "  # Arg: bucket **bucket name** [str],\n",
        "  #      key **object key** [str]\n",
        "  # Returns: objs **objects in key**\n",
        "  def scanFolder(self, bucket, key):\n",
        "    objs = []\n",
        "    pages = self.paginator.paginate(Bucket=bucket, Prefix=key)\n",
        "    for page in pages:\n",
        "      for content in page['Contents']:\n",
        "        if not content['Key'].endswith(\"/\"):\n",
        "          objs.append(content['Key'])\n",
        "    return objs\n",
        "\n",
        "  # Look up an object\n",
        "  # Arg: bucket [string] **S3 bucket to look in**\n",
        "  #      key [string] **key to look in**\n",
        "  #      id [str] **lookup object id**\n",
        "  # Returns: key **object key if it exists**\n",
        "  def lookupObj(self, bucket, key, query, group):\n",
        "    keys = []\n",
        "    objs = self.scanFolder(bucket, key)\n",
        "    for obj in objs:\n",
        "      lookup = re.search(query, obj)\n",
        "      if lookup:\n",
        "        keys.append(lookup.group(group))\n",
        "    return keys"
      ],
      "metadata": {
        "id": "mrruG1N1r7wF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SaveStockData Class"
      ],
      "metadata": {
        "id": "AOXS2mus_OSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SaveStockData:\n",
        "  def __init__(self):\n",
        "    self.s3Helper = AccessS3()\n",
        "\n",
        "  def createMetaPath(self, newEntry, network):\n",
        "    id = newEntry[\"id\"]\n",
        "    year = newEntry[\"published_parsed\"][0]\n",
        "    month = newEntry[\"published_parsed\"][1]\n",
        "    day = newEntry[\"published_parsed\"][2]\n",
        "    metaKey = \"metadata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
        "    return metaKey\n",
        "\n",
        "  def createTextPath(self, newEntry, network):\n",
        "    id = newEntry[\"id\"]\n",
        "    year = newEntry[\"published_parsed\"][0]\n",
        "    month = newEntry[\"published_parsed\"][1]\n",
        "    day = newEntry[\"published_parsed\"][2]\n",
        "    textKey = \"textdata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
        "    return textKey\n",
        "\n",
        "  def collectMetaData(self, newEntry, feed):\n",
        "    title = newEntry[\"title\"]\n",
        "    link = newEntry[\"link\"]\n",
        "    date = newEntry[\"published\"]\n",
        "    sponsor = newEntry[\"metadata_sponsored\"]\n",
        "    metaType = newEntry[\"metadata_type\"]\n",
        "    metaData = json.dumps({\"title\":title,\"link\":link,\"date\":date,\"feed\":feed,\"sponsor\":sponsor,\"type\":metaType})\n",
        "    return metaData\n",
        "\n",
        "  def collectTextData(self, newEntry):\n",
        "    textData = json.dumps(newEntry[\"text\"])\n",
        "    return textData\n",
        "\n",
        "  def saveData(self, data, bucket, key):\n",
        "    self.s3Helper.saveObj(data, bucket, key)\n",
        "    return 0"
      ],
      "metadata": {
        "id": "CH1f-duM_OI_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save New Articles to S3"
      ],
      "metadata": {
        "id": "QUxoCxQb6OJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the text data and metadata for a new entry\n",
        "# Arg: newEntry [dict] **new article to be saved**\n",
        "#      network [string] **network that published the RSS feed**\n",
        "#      feed [string] **name of the RSS feed**\n",
        "#      bucket [string] **S3 bucket where data is stored**\n",
        "def saveData(newEntry, network, feed, bucket):\n",
        "  stockSaver = SaveStockData()\n",
        "\n",
        "  # scrape and add the text to the entry\n",
        "  entryURL = newEntry[\"link\"]\n",
        "  soup = scraper(entryURL)\n",
        "  result = textExtract(soup)\n",
        "  newEntry[\"text\"] = result\n",
        "\n",
        "  # create object path\n",
        "  metaKey = stockSaver.createMetaPath(newEntry, network)\n",
        "  textKey = stockSaver.createTextPath(newEntry, network)\n",
        "\n",
        "  # collect metadata and text data\n",
        "  metaData = stockSaver.collectMetaData(newEntry, feed)\n",
        "  textData = stockSaver.collectTextData(newEntry)\n",
        "\n",
        "  # upload metadata\n",
        "  stockSaver.saveData(metaData, bucket, metaKey)\n",
        "  # upload text data\n",
        "  stockSaver.saveData(textData, bucket, textKey)\n",
        "\n",
        "  return 0"
      ],
      "metadata": {
        "id": "9tt1okDhwIGA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save new articles to S3 and update old ones\n",
        "# Arg: entries [list of dictionaries] **entries gathered from feed**\n",
        "#      network [string] **network that published the RSS feed**\n",
        "#      feed [string] **name of the RSS feed**\n",
        "#      bucket [string] **S3 bucket where data is stored**\n",
        "#      folder [string] **folder where data is stored**\n",
        "def saveNewEntries(entries, network, feed, bucket):\n",
        "  s3Helper = AccessS3()\n",
        "  count = 0\n",
        "  print(\"This feed has {} articles\".format(len(entries)))\n",
        "  for entry in entries:\n",
        "    # get the article id\n",
        "    id = entry[\"id\"]\n",
        "    # look up id to see if it exists already (i.e. has been saved before)\n",
        "    metaQuery = \"^metadata\\/(.*\"+id+\"\\.[\\D]{4})$\"\n",
        "    keys = s3Helper.lookupObj(bucket, \"metadata\", metaQuery, 1)\n",
        "\n",
        "    # if metakey is empty (i.e. it does not exist),\n",
        "    if not keys:\n",
        "      # then save it to s3\n",
        "      print(\"Saving new article id {}:\".format(id))\n",
        "      saveData(entry, network, feed, bucket)\n",
        "      count += 1\n",
        "\n",
        "    # otherwise it exists\n",
        "    else:\n",
        "      metaDates = []\n",
        "      for key in keys:\n",
        "        metaKey = \"metadata/\"+key\n",
        "        metaData = (json.loads(s3Helper.getObj(bucket, metaKey)['Body'].read().decode()))\n",
        "        metaDates.append(metaData[\"date\"])\n",
        "      # convert dates to comparable datetime objects\n",
        "      existingDates = convertDates(metaDates)\n",
        "      newestDate = max(existingDates)\n",
        "      entryDate = convertDates([entry[\"published\"]])[0]\n",
        "\n",
        "      # if the article has been updated,\n",
        "      if entryDate > newestDate:\n",
        "        print(\"Updating article id: {}\".format(id))\n",
        "        for key in keys:\n",
        "          # delete the old one(s)\n",
        "          metaKey = \"metadata/\"+key\n",
        "          textKey = \"textdata/\"+key\n",
        "          #print(\"Deleted {}\".format(metaKey))\n",
        "          #print(\"Deleted {}\".format(textKey))\n",
        "          s3Helper.deleteObj(bucket, metaKey)\n",
        "          s3Helper.deleteObj(bucket, textKey)\n",
        "        # and save the new one\n",
        "        saveData(entry, network, feed, bucket)\n",
        "        count += 1\n",
        "\n",
        "      # otherwise check for existing duplicates\n",
        "      elif len(existingDates) > 1:\n",
        "        print(\"Deleting duplicate articles id: {}\".format(id))\n",
        "        for key, existingDate in zip(keys, existingDates):\n",
        "          if not existingDate==newestDate:\n",
        "            # and delete the older ones\n",
        "            metaKey = \"metadata/\"+key\n",
        "            textKey = \"textdata/\"+key\n",
        "            #print(\"Deleted {}\".format(metaKey))\n",
        "            #print(\"Deleted {}\".format(textKey))\n",
        "            s3Helper.deleteObj(bucket, metaKey)\n",
        "            s3Helper.deleteObj(bucket, textKey)\n",
        "  # report the total number of articles that have been saved\n",
        "  if count==0:\n",
        "    print('No new entries for {}'.format(feed))\n",
        "  else:\n",
        "    print(\"{} new articles for {}\".format(count, feed))\n",
        "  return 0"
      ],
      "metadata": {
        "id": "F6_QRSNJLmok"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "b2xBf8an5Pvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(event, context):\n",
        "  bucket = os.environ[\"bucket\"]\n",
        "  rssKey = os.environ[\"rssKey\"]\n",
        "  s3Helper = AccessS3()\n",
        "  # Retrieve list of RSS feeds to gather articles from\n",
        "  obj = s3Helper.getObj(bucket, rssKey)\n",
        "  rssList = json.loads(obj[\"Body\"].read().decode())\n",
        "  # Gather articles from the feeds and save new ones to S3\n",
        "  count = 0\n",
        "  for rss in rssList:\n",
        "    rssNetwork = rss[\"network\"]\n",
        "    rssFeed = rss[\"feed\"]\n",
        "    rssURL = rss[\"url\"]\n",
        "    # Fetch entries from the current RSS feed\n",
        "    print(\"Currently scraping from {}\".format(rssFeed))\n",
        "    entries = fetchRSS(rssURL)\n",
        "    # Save new entries to S3\n",
        "    saveNewEntries(entries, rssNetwork, rssFeed, bucket)\n",
        "  return {\n",
        "      'statusCode': 200\n",
        "  }"
      ],
      "metadata": {
        "id": "WnDjLKc35Qa4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "VjSuiHoVI30z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "  # import text and test\n",
        "  result = main(None, None)\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "B6Oha02K5hjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64678333-1cc1-42e7-c768-f6c6b3f21d85",
        "collapsed": true
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently scraping from Top News\n",
            "This feed has 30 articles\n",
            "No new entries for Top News\n",
            "Currently scraping from Earnings\n",
            "This feed has 30 articles\n",
            "No new entries for Earnings\n",
            "Currently scraping from Economy\n",
            "This feed has 30 articles\n",
            "No new entries for Economy\n",
            "Currently scraping from Finance\n",
            "This feed has 30 articles\n",
            "No new entries for Finance\n",
            "Currently scraping from Tech\n",
            "This feed has 30 articles\n",
            "No new entries for Tech\n",
            "Currently scraping from Investing\n",
            "This feed has 30 articles\n",
            "No new entries for Investing\n",
            "{'statusCode': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-CTageP_w18"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}