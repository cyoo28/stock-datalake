{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK+Fn1bNlhLziIdCgf1RB4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iyoo2018/findatalake/blob/master/scrapeRSS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Exclusive to Colab"
      ],
      "metadata": {
        "id": "uB_hhdZs40Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "XgNE46dr7JFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  import sys\n",
        "  sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXOvubHx7Nnd",
        "outputId": "018993ba-e009-4ba3-9179-11cd278564cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3"
      ],
      "metadata": {
        "id": "ZGyFLxclb4dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "  os.environ['AWS_CONFIG_FILE']=\"/content/gdrive/My Drive/cred-stockdata.txt\"\n",
        "  os.environ['AWS_CONFIG_FILE']\n",
        "\n",
        "  s = boto3.Session()\n",
        "  c = s.client(\"s3\")\n",
        "\n",
        "  import json\n",
        "  os.environ[\"bucket\"] = \"026090555438-stockdata\"\n",
        "  os.environ[\"key\"] = \"rssList.json\""
      ],
      "metadata": {
        "id": "VbzRoqao8Z87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "  # CNBC Top News\n",
        "  topNews = {\"network\":\"CNBC\", \"feed\":\"Top News\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114\"}\n",
        "\n",
        "  # Earnings\n",
        "  earning = {\"network\":\"CNBC\", \"feed\":\"Earnings\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839135\"}\n",
        "\n",
        "  # Economy\n",
        "  economy = {\"network\":\"CNBC\", \"feed\":\"Economy\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=20910258\"}\n",
        "\n",
        "  # Finance\n",
        "  finance = {\"network\":\"CNBC\", \"feed\":\"Finance\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664\"}\n",
        "\n",
        "  # Tech\n",
        "  tech = {\"network\":\"CNBC\", \"feed\":\"Tech\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910\"}\n",
        "\n",
        "  # Investing\n",
        "  invest = {\"network\":\"CNBC\", \"feed\":\"Investing\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069\"}\n",
        "\n",
        "  rssList = json.dumps([topNews, earning, economy, finance, tech, invest])\n",
        "\n",
        "  c.put_object(\n",
        "  Body=rssList,\n",
        "  Bucket='026090555438-stockdata',\n",
        "  Key=\"rssList.json\"\n",
        ")"
      ],
      "metadata": {
        "id": "ilmY-YssHd1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fetchRSS"
      ],
      "metadata": {
        "id": "jxybr4-t43Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "\n",
        "# Fetch the articles from an RSS feed\n",
        "# Arg: source [string] *contains RSS feed link*\n",
        "# Returns: entries [list of dictionaries] *contains articles in RSS feed*\n",
        "def fetchRSS(source):\n",
        "    # fetch the RSS feed\n",
        "    RSS = feedparser.parse(source)\n",
        "    # extract the entries from the retrieved feed\n",
        "    entries = RSS.entries\n",
        "    return entries"
      ],
      "metadata": {
        "id": "OSvIKGOu43Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# checkID"
      ],
      "metadata": {
        "id": "f77DmOOV5B8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a file already exists\n",
        "# Arg: bucket [string] **S3 bucket**\n",
        "#      path [string] **path within the bucket**\n",
        "#      s3 [boto3 s3 client object]\n",
        "# Returns: boolean **indicator as to whether or not the file exists**\n",
        "def fileExists(bucket, path, s3):\n",
        "    # check if the file exists by requesting its metadata\n",
        "    try:\n",
        "        s3.head_object(Bucket=bucket, Key=path)\n",
        "        # if the file exists, return True\n",
        "        return True\n",
        "    # otherwise return False\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Check which entries are new\n",
        "# Arg: entries [list of dictionaries] **current entries published to an RSS feed**\n",
        "#      s3 [boto3 s3 client object]\n",
        "#      network [string] **network that published RSS feed**\n",
        "# Returns: newEntries [list of dictionaries] **new entries that haven't been seen**\n",
        "def checkID(entries, s3, network):\n",
        "    newEntries = []\n",
        "    for entry in entries:\n",
        "        # get the article id\n",
        "        id = entry[\"id\"]\n",
        "        # get the date the article was published\n",
        "        date = entry[\"published_parsed\"]\n",
        "        year = date[0]\n",
        "        month = date[1]\n",
        "        day = date[2]\n",
        "        # search to see if the file already exists\n",
        "        bucket = \"026090555438-stockdata\"\n",
        "        path = \"metadata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
        "        # if it doesn't then that means this article is new\n",
        "        if not fileExists(bucket, path, s3):\n",
        "            newEntries.append(entry)\n",
        "    return newEntries"
      ],
      "metadata": {
        "id": "AyGR4OeG470U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# parseHTML"
      ],
      "metadata": {
        "id": "HLcizzAc5FV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Scrape an article using the url\n",
        "# Arg: url [string] **contains url for the article**\n",
        "# Returns: soup [BeautifulSoup object]\n",
        "def scraper(url):\n",
        "  # scrape the data from the url using requests and BeautifulSoup\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "  return soup\n",
        "\n",
        "# Extract text from the scraped/parsed article\n",
        "# Arg: soup [BeautifulSoup object]\n",
        "# Returns: result [string] **contains the text of the article**\n",
        "def textExtract(soup):\n",
        "  # extract the data corresponding to the article from the soup\n",
        "  articleGroups = soup.find_all(\"div\", {\"class\": \"group\"})\n",
        "  sections = []\n",
        "  for group in articleGroups:\n",
        "    section = group.find_all(\"p\")\n",
        "    if section:\n",
        "      sections.append(section)\n",
        "  # extract the text of the article\n",
        "  text = []\n",
        "  for section in sections:\n",
        "    for para in section:\n",
        "      text.append(para.get_text())\n",
        "  # return the article joined as a single string\n",
        "  return (\"\\n\").join(text)\n",
        "\n",
        "# Obtain author of the article\n",
        "# Arg: soup [BeautifulSoup object]\n",
        "# Returns: name [string] **author's name**\n",
        "#          profile [string] **url to author's profile**\n",
        "def getAuthor(soup):\n",
        "  authorInfo = soup.find_all(\"a\", {\"class\": \"Author-authorName\"})\n",
        "  print(authorInfo)\n",
        "  name = authorInfo[0].getText()\n",
        "  profile = authorInfo[0][\"href\"]\n",
        "  return name, profile"
      ],
      "metadata": {
        "id": "qbyJkagP5H0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# saveS3"
      ],
      "metadata": {
        "id": "sL-MPju05L-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save new files\n",
        "# Arg: newEntries [list of dictionaries] **contains new articles to be saved**\n",
        "#      s3 [boto3 s3 client object]\n",
        "#      network [string] **name of network that published RSS feed**\n",
        "#      feed [string] **name of RSS feed**\n",
        "def saves3(newEntries, s3, network, feed):\n",
        "    for newEntry in newEntries:\n",
        "      # get info for file save path\n",
        "      id = newEntry[\"id\"]\n",
        "      year = newEntry[\"published_parsed\"][0]\n",
        "      month = newEntry[\"published_parsed\"][1]\n",
        "      day = newEntry[\"published_parsed\"][2]\n",
        "      metapath = \"metadata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
        "      textpath = \"textdata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
        "\n",
        "      # collect and upload metadata\n",
        "      #authorName = newEntry[\"author-name\"]\n",
        "      #authorProfile = newEntry[\"author-profile\"]\n",
        "      title = newEntry[\"title\"]\n",
        "      link = newEntry[\"link\"]\n",
        "      date = newEntry[\"published\"]\n",
        "      sponsor = newEntry[\"metadata_sponsored\"]\n",
        "      metatype = newEntry[\"metadata_type\"]\n",
        "      #metadata = {\"author-name\":authorName,\"author-profile\":authorProfile,\"title\":title,\"link\":link,\"date\":date,\"feed\":feed}\n",
        "      metadata = json.dumps({\"title\":title,\"link\":link,\"date\":date,\"feed\":feed,\"sponsor\":sponsor,\"type\":metatype})\n",
        "      s3.put_object(\n",
        "        Body=metadata,\n",
        "        Bucket='026090555438-stockdata',\n",
        "        Key=metapath\n",
        "      )\n",
        "      # upload text data\n",
        "      textdata = json.dumps(newEntry[\"text\"])\n",
        "      s3.put_object(\n",
        "        Body=textdata,\n",
        "        Bucket='026090555438-stockdata',\n",
        "        Key=textpath\n",
        "      )\n",
        "    return 0"
      ],
      "metadata": {
        "id": "mkA64I2V5LZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "b2xBf8an5Pvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import boto3\n",
        "\n",
        "# event is a dictionary containing the feed name and URL\n",
        "# assume RSS feed is from CNBC\n",
        "# assume event is given as json and contains 1 dictionary for 1 RSS feed\n",
        "def main(event, context):\n",
        "    session = boto3.Session()\n",
        "    s3 = session.client('s3')\n",
        "    bucket = os.environ[\"bucket\"]\n",
        "    key = os.environ[\"key\"]\n",
        "\n",
        "    rssList = json.loads(s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read().decode())\n",
        "\n",
        "    count = 0\n",
        "    for rss in rssList:\n",
        "      rssNetwork = rss[\"network\"]\n",
        "      rssFeed = rss[\"feed\"]\n",
        "      rssURL = rss[\"url\"]\n",
        "\n",
        "      print(\"Currently scraping from {}\".format(rssFeed))\n",
        "      print(\"Step 1: fetch entries from RSS feed\")\n",
        "      entries = fetchRSS(rssURL)\n",
        "\n",
        "      print(\"Step 2: check for new entries\")\n",
        "      newEntries = checkID(entries, s3, rssNetwork)\n",
        "      # if no new entries there is nothing else that needs to be done\n",
        "      if not newEntries:\n",
        "          print('No new entries for {}'.format(rssFeed))\n",
        "          continue\n",
        "      print(\"{} new articles for {}\".format(len(newEntries),rssFeed))\n",
        "\n",
        "      print(\"Step 3: scrape and parse text for new entries\")\n",
        "      saved = 0\n",
        "      for newEntry in newEntries:\n",
        "          entryURL = newEntry[\"link\"]\n",
        "          soup = scraper(entryURL)\n",
        "          result = textExtract(soup)\n",
        "          newEntry[\"text\"] = result\n",
        "          #authorName, authorProfile = getAuthor(soup)\n",
        "          #newEntry[\"author-name\"] = authorName\n",
        "          #newEntry[\"author-profile\"] = authorProfile\n",
        "          saved += 1\n",
        "          if saved%5==0:\n",
        "            print(\"Scraped {} articles\".format(saved))\n",
        "\n",
        "      print(\"Step 4: save to S3\")\n",
        "      saves3(newEntries, s3, rssNetwork, rssFeed)\n",
        "      count += len(newEntries)\n",
        "\n",
        "    print('{} new entries have been saved'.format(count))\n",
        "    return {\n",
        "        'statusCode': 200\n",
        "    }"
      ],
      "metadata": {
        "id": "WnDjLKc35Qa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test to see that it works"
      ],
      "metadata": {
        "id": "VjSuiHoVI30z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "  # import text and test\n",
        "  result = main(None, None)\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "B6Oha02K5hjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88db44f-fe40-4399-c331-a7511ebbd143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently scraping from Top News\n",
            "Step 1: fetch entries from RSS feed\n",
            "Step 2: check for new entries\n",
            "No new entries for Top News\n",
            "Currently scraping from Earnings\n",
            "Step 1: fetch entries from RSS feed\n",
            "Step 2: check for new entries\n",
            "No new entries for Earnings\n",
            "Currently scraping from Economy\n",
            "Step 1: fetch entries from RSS feed\n",
            "Step 2: check for new entries\n",
            "No new entries for Economy\n",
            "Currently scraping from Finance\n",
            "Step 1: fetch entries from RSS feed\n",
            "Step 2: check for new entries\n",
            "No new entries for Finance\n",
            "Currently scraping from Tech\n",
            "Step 1: fetch entries from RSS feed\n",
            "Step 2: check for new entries\n",
            "No new entries for Tech\n",
            "Currently scraping from Investing\n",
            "Step 1: fetch entries from RSS feed\n",
            "Step 2: check for new entries\n",
            "No new entries for Investing\n",
            "0 new entries have been saved\n",
            "{'statusCode': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-CTageP_w18"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}