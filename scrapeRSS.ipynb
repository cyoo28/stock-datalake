{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpFas7WYqwKuz0c7uC2mXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iyoo2018/findatalake/blob/master/scrapeRSS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Exclusive to Colab"
      ],
      "metadata": {
        "id": "uB_hhdZs40Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  import sys\n",
        "  sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"
      ],
      "metadata": {
        "id": "XgNE46dr7JFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb4933a-a940-4019-91c7-97f433b932a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "  os.environ['AWS_CONFIG_FILE']=\"/content/gdrive/My Drive/cred-stockdata.txt\"\n",
        "  os.environ[\"bucket\"] = \"026090555438-stockdata\"\n",
        "  os.environ[\"rssKey\"] = \"rssList.json\""
      ],
      "metadata": {
        "id": "VbzRoqao8Z87"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# CNBC Top News\n",
        "topNews = {\"network\":\"CNBC\", \"feed\":\"Top News\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114\"}\n",
        "# Earnings\n",
        "earning = {\"network\":\"CNBC\", \"feed\":\"Earnings\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839135\"}\n",
        "# Economy\n",
        "economy = {\"network\":\"CNBC\", \"feed\":\"Economy\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=20910258\"}\n",
        "# Finance\n",
        "finance = {\"network\":\"CNBC\", \"feed\":\"Finance\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664\"}\n",
        "# Tech\n",
        "tech = {\"network\":\"CNBC\", \"feed\":\"Tech\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910\"}\n",
        "# Investing\n",
        "invest = {\"network\":\"CNBC\", \"feed\":\"Investing\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069\"}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ilmY-YssHd1H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "13c5e683-bad8-44f9-9bca-414ccf02235d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# CNBC Top News\\ntopNews = {\"network\":\"CNBC\", \"feed\":\"Top News\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114\"}\\n# Earnings\\nearning = {\"network\":\"CNBC\", \"feed\":\"Earnings\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839135\"}\\n# Economy\\neconomy = {\"network\":\"CNBC\", \"feed\":\"Economy\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=20910258\"}\\n# Finance\\nfinance = {\"network\":\"CNBC\", \"feed\":\"Finance\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664\"}\\n# Tech\\ntech = {\"network\":\"CNBC\", \"feed\":\"Tech\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910\"}\\n# Investing\\ninvest = {\"network\":\"CNBC\", \"feed\":\"Investing\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069\"}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ],
      "metadata": {
        "id": "-aHDFHqfNEeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "import boto3\n",
        "import datetime"
      ],
      "metadata": {
        "id": "N52GAxVRMm6N"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape RSS Feeds"
      ],
      "metadata": {
        "id": "OwjSigWoMx_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the entries from an RSS feed\n",
        "# Arg: source [string] *contains RSS feed link*\n",
        "# Returns: entries [list of dictionaries] *contains entries in RSS feed*\n",
        "def fetchRSS(source):\n",
        "  # fetch the RSS feed\n",
        "  RSS = feedparser.parse(source)\n",
        "  # extract the entries from the retrieved feed\n",
        "  entries = RSS.entries\n",
        "  return entries"
      ],
      "metadata": {
        "id": "OSvIKGOu43Cc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse HTML"
      ],
      "metadata": {
        "id": "ezJS6W4UNLK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape an entry using the url\n",
        "# Arg: url [string] **contains url for the entry**\n",
        "# Returns: soup [BeautifulSoup object]\n",
        "def scraper(url):\n",
        "  # scrape the data from the url using requests and BeautifulSoup\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "  return soup\n",
        "\n",
        "# Extract text from the scraped/parsed entry\n",
        "# Arg: soup [BeautifulSoup object]\n",
        "# Returns: result [string] **contains the text of the entry**\n",
        "def textExtract(soup):\n",
        "  # extract the data corresponding to the entry from the soup\n",
        "  entryGroups = soup.find_all(\"div\", {\"class\": \"group\"})\n",
        "  sections = []\n",
        "  for group in entryGroups:\n",
        "    section = group.find_all(\"p\")\n",
        "    if section:\n",
        "      sections.append(section)\n",
        "  # extract the text of the entry\n",
        "  text = []\n",
        "  for section in sections:\n",
        "    for para in section:\n",
        "      text.append(para.get_text())\n",
        "  # return the entry joined as a single string\n",
        "  return (\"\\n\").join(text)"
      ],
      "metadata": {
        "id": "ktyhYTtANJhh"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get publication dates"
      ],
      "metadata": {
        "id": "jn0wyZQGrT2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get publication dates of files\n",
        "# Arg: bucket **bucket name** [str],\n",
        "#      keys **list of keys that require dates** [list of str]\n",
        "# Returns: list of dates [list of str]\n",
        "def getDates(bucket, keys, s3Helper):\n",
        "  dates = []\n",
        "  for key in keys:\n",
        "    date = (json.loads(s3Helper.getObj(bucket, key)['Body'].read().decode()))\n",
        "    dates.append(date[\"date\"])\n",
        "  return dates"
      ],
      "metadata": {
        "id": "aps_rImQrTqK"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert publication dates"
      ],
      "metadata": {
        "id": "6Ei3HbWBPlZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert publication dates of files\n",
        "# Arg: unstrDates **unstructed dates** [list of str]\n",
        "# Returns: list of structured dates [list of str]\n",
        "def convertDates(unstrDates):\n",
        "  dateFormat = \"%a, %d %b %Y %H:%M:%S %Z\"\n",
        "  return [datetime.datetime.strptime(unstrDate, dateFormat) for unstrDate in unstrDates]"
      ],
      "metadata": {
        "id": "SrBZJVB8PlMV"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AccessS3 Class"
      ],
      "metadata": {
        "id": "iAXvRUXENIzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AccessS3:\n",
        "  def __init__(self):\n",
        "    session = boto3.Session()\n",
        "    self.s3 = session.client('s3')\n",
        "    self.paginator = self.s3.get_paginator('list_objects_v2')\n",
        "\n",
        "  # Get an object\n",
        "  # Arg: bucket **bucket name** [str],\n",
        "  #      key **object key** [str],\n",
        "  # Returns: object\n",
        "  def getObj(self, bucket, key):\n",
        "    return self.s3.get_object(Bucket=bucket, Key=key)\n",
        "\n",
        "  # Delete an object\n",
        "  # Arg: bucket **bucket name** [str],\n",
        "  #      key **object key** [str]\n",
        "  def deleteObj(self, bucket, key):\n",
        "    self.s3.delete_object(Bucket=bucket, Key=key)\n",
        "    print(\"Deleted object at {}\".format(key))\n",
        "    return 0\n",
        "\n",
        "  # Save an object\n",
        "  # Arg: data **data to be saved**\n",
        "  #      bucket **bucket name** [str],\n",
        "  #      key **object key** [str]\n",
        "  def saveObj(self, data, bucket, key):\n",
        "    self.s3.put_object(\n",
        "      Body=data,\n",
        "      Bucket=bucket,\n",
        "      Key=key\n",
        "    )\n",
        "    print(\"Saved object at {}\".format(key))\n",
        "    return 0\n",
        "\n",
        "  # Look at objects contained in a key\n",
        "  # Arg: bucket **bucket name** [str],\n",
        "  #      key **object key** [str]\n",
        "  # Returns: objs **objects in key**\n",
        "  def scanFolder(self, bucket, key):\n",
        "    objs = []\n",
        "    pages = self.paginator.paginate(Bucket=bucket, Prefix=key)\n",
        "    for page in pages:\n",
        "      for content in page['Contents']:\n",
        "        if not content['Key'].endswith(\"/\"):\n",
        "          objs.append(content['Key'])\n",
        "    return objs\n",
        "\n",
        "  # Look up an object\n",
        "  # Arg: bucket [string] **S3 bucket to look in**\n",
        "  #      key [string] **key to look in**\n",
        "  #      id [str] **lookup object id**\n",
        "  # Returns: key **object key if it exists**\n",
        "  def lookupObj(self, bucket, key, query, group):\n",
        "    keys = []\n",
        "    objs = self.scanFolder(bucket, key)\n",
        "    for obj in objs:\n",
        "      lookup = re.search(query, obj)\n",
        "      if lookup:\n",
        "        keys.append(lookup.group(group))\n",
        "    return keys"
      ],
      "metadata": {
        "id": "mrruG1N1r7wF"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SaveStockData Class"
      ],
      "metadata": {
        "id": "AOXS2mus_OSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SaveStockData:\n",
        "  def __init__(self, s3Helper):\n",
        "    self.s3Helper = s3Helper\n",
        "\n",
        "  def createMetaPath(self, newEntry, network):\n",
        "    id = newEntry[\"id\"]\n",
        "    year = newEntry[\"published_parsed\"][0]\n",
        "    month = newEntry[\"published_parsed\"][1]\n",
        "    day = newEntry[\"published_parsed\"][2]\n",
        "    metaKey = \"metadata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
        "    return metaKey\n",
        "\n",
        "  def createTextPath(self, newEntry, network):\n",
        "    id = newEntry[\"id\"]\n",
        "    year = newEntry[\"published_parsed\"][0]\n",
        "    month = newEntry[\"published_parsed\"][1]\n",
        "    day = newEntry[\"published_parsed\"][2]\n",
        "    textKey = \"textdata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
        "    return textKey\n",
        "\n",
        "  def collectMetaData(self, newEntry, feed):\n",
        "    title = newEntry[\"title\"]\n",
        "    link = newEntry[\"link\"]\n",
        "    date = newEntry[\"published\"]\n",
        "    sponsor = newEntry[\"metadata_sponsored\"]\n",
        "    metaType = newEntry[\"metadata_type\"]\n",
        "    metaData = json.dumps({\"title\":title,\"link\":link,\"date\":date,\"feed\":feed,\"sponsor\":sponsor,\"type\":metaType})\n",
        "    return metaData\n",
        "\n",
        "  def collectTextData(self, newEntry):\n",
        "    textData = json.dumps(newEntry[\"text\"])\n",
        "    return textData\n",
        "\n",
        "  def saveData(self, data, bucket, key):\n",
        "    self.s3Helper.saveObj(data, bucket, key)\n",
        "    return 0"
      ],
      "metadata": {
        "id": "CH1f-duM_OI_"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save New Entries to S3"
      ],
      "metadata": {
        "id": "QUxoCxQb6OJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the text data and metadata for a new entry\n",
        "# Arg: newEntry [dict] **new entry to be saved**\n",
        "#      network [string] **network that published the RSS feed**\n",
        "#      feed [string] **name of the RSS feed**\n",
        "#      bucket [string] **S3 bucket where data is stored**\n",
        "def saveSingleEntry(newEntry, network, feed, bucket, s3Helper):\n",
        "  stockSaver = SaveStockData(s3Helper)\n",
        "\n",
        "  # scrape and add the text to the entry\n",
        "  entryURL = newEntry[\"link\"]\n",
        "  soup = scraper(entryURL)\n",
        "  result = textExtract(soup)\n",
        "  newEntry[\"text\"] = result\n",
        "\n",
        "  # create object path\n",
        "  metaKey = stockSaver.createMetaPath(newEntry, network)\n",
        "  textKey = stockSaver.createTextPath(newEntry, network)\n",
        "\n",
        "  # collect metadata and text data\n",
        "  metaData = stockSaver.collectMetaData(newEntry, feed)\n",
        "  textData = stockSaver.collectTextData(newEntry)\n",
        "\n",
        "  # upload metadata\n",
        "  stockSaver.saveData(metaData, bucket, metaKey)\n",
        "  # upload text data\n",
        "  stockSaver.saveData(textData, bucket, textKey)\n",
        "\n",
        "  return 0"
      ],
      "metadata": {
        "id": "9tt1okDhwIGA"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save new entries to S3 and update old ones\n",
        "# Arg: entries [list of dictionaries] **entries gathered from feed**\n",
        "#      network [string] **network that published the RSS feed**\n",
        "#      feed [string] **name of the RSS feed**\n",
        "#      bucket [string] **S3 bucket where data is stored**\n",
        "#      folder [string] **folder where data is stored**\n",
        "def saveNewEntries(entries, network, feed, bucket):\n",
        "  s3Helper = AccessS3()\n",
        "  count = 0\n",
        "  print(\"This feed has {} entries\".format(len(entries)))\n",
        "  # get the existing entries from s3\n",
        "  existKeys = s3Helper.scanFolder(bucket, \"metadata\")\n",
        "  for entry in entries:\n",
        "    # get the entry id\n",
        "    entryID = entry[\"id\"]\n",
        "    # if the entry id does not already exist\n",
        "    if not any(entryID in existKey for existKey in existKeys):\n",
        "      # then save it to s3\n",
        "      print(\"Saving new entry id {}:\".format(id))\n",
        "      saveSingleEntry(entry, network, feed, bucket, s3Helper)\n",
        "      count += 1\n",
        "    # otherwise it already exists\n",
        "    else:\n",
        "      # search for matching entries\n",
        "      matchKeys = [existKey.split(\"/\", 1)[1] for existKey in existKeys if entryID in existKey]\n",
        "      metaKeys = [\"metadata/\"+matchKey for matchKey in matchKeys]\n",
        "      textKeys = [\"textdata/\"+matchKey for matchKey in matchKeys]\n",
        "      # retrieve and convert dates to comparable datetime objects\n",
        "      matchDates = getDates(bucket, metaKeys, s3Helper)\n",
        "      existDates = convertDates(matchDates)\n",
        "      latestDate = max(existDates)\n",
        "      entryDate = convertDates([entry[\"published\"]])[0]\n",
        "      # if the entry has been updated,\n",
        "      if entryDate > latestDate:\n",
        "        print(\"Updating entry id: {}\".format(id))\n",
        "        for metaKey, textKey in zip(metaKeys, textKeys):\n",
        "          # delete the old one(s)\n",
        "          #print(\"Deleted {}\".format(metaKey))\n",
        "          #print(\"Deleted {}\".format(textKey))\n",
        "          s3Helper.deleteObj(bucket, metaKey)\n",
        "          s3Helper.deleteObj(bucket, textKey)\n",
        "        # and save the new one\n",
        "        saveSingleEntry(entry, network, feed, bucket, s3Helper)\n",
        "        count += 1\n",
        "      # otherwise check for duplicates\n",
        "      elif len(matchKeys) > 1:\n",
        "        print(\"Deleting duplicate entries id: {}\".format(id))\n",
        "        for metaKey, textKey, existDate in zip(metaKeys, textKeys, existDates):\n",
        "          if not existDate==latestDate:\n",
        "            # and delete the older ones\n",
        "            #print(\"Deleted {}\".format(metaKey))\n",
        "            #print(\"Deleted {}\".format(textKey))\n",
        "            s3Helper.deleteObj(bucket, metaKey)\n",
        "            s3Helper.deleteObj(bucket, textKey)\n",
        "  # report the total number of entries that have been saved\n",
        "  if count==0:\n",
        "    print('No new entries for {}'.format(feed))\n",
        "  else:\n",
        "    print(\"{} new entries for {}\".format(count, feed))\n",
        "  return 0"
      ],
      "metadata": {
        "id": "F6_QRSNJLmok"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "b2xBf8an5Pvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(event, context):\n",
        "  bucket = os.environ[\"bucket\"]\n",
        "  rssKey = os.environ[\"rssKey\"]\n",
        "  s3Helper = AccessS3()\n",
        "  # Retrieve list of RSS feeds to gather entries from\n",
        "  obj = s3Helper.getObj(bucket, rssKey)\n",
        "  rssList = json.loads(obj[\"Body\"].read().decode())\n",
        "  # Gather entries from the feeds and save new ones to S3\n",
        "  count = 0\n",
        "  for rss in rssList:\n",
        "    rssNetwork = rss[\"network\"]\n",
        "    rssFeed = rss[\"feed\"]\n",
        "    rssURL = rss[\"url\"]\n",
        "    # Fetch entries from the current RSS feed\n",
        "    print(\"Currently scraping from {}\".format(rssFeed))\n",
        "    entries = fetchRSS(rssURL)\n",
        "    # Save new entries to S3\n",
        "    saveNewEntries(entries, rssNetwork, rssFeed, bucket)\n",
        "  return {\n",
        "      'statusCode': 200\n",
        "  }"
      ],
      "metadata": {
        "id": "WnDjLKc35Qa4"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "VjSuiHoVI30z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "  # import text and test\n",
        "  result = main(None, None)\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "B6Oha02K5hjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959ac6f7-817c-493f-d9ca-d8cda950882d",
        "collapsed": true
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently scraping from Top News\n",
            "This feed has 30 entries\n",
            "No new entries for Top News\n",
            "Currently scraping from Earnings\n",
            "This feed has 30 entries\n",
            "No new entries for Earnings\n",
            "Currently scraping from Economy\n",
            "This feed has 30 entries\n",
            "No new entries for Economy\n",
            "Currently scraping from Finance\n",
            "This feed has 30 entries\n",
            "No new entries for Finance\n",
            "Currently scraping from Tech\n",
            "This feed has 30 entries\n",
            "No new entries for Tech\n",
            "Currently scraping from Investing\n",
            "This feed has 30 entries\n",
            "No new entries for Investing\n",
            "{'statusCode': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-CTageP_w18"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}