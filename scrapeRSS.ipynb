{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy7jlcKCVqk/CrziZZ6lSn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iyoo2018/findatalake/blob/master/scrapeRSS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Exclusive to Colab"
      ],
      "metadata": {
        "id": "uB_hhdZs40Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  import sys\n",
        "  sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"
      ],
      "metadata": {
        "id": "XgNE46dr7JFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2896d1c-2da5-4da2-e5f3-93b8a8734a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "  os.environ['AWS_CONFIG_FILE']=\"/content/gdrive/My Drive/cred-stockdata.txt\"\n",
        "  os.environ[\"bucket\"] = \"026090555438-stockdata\"\n",
        "  os.environ[\"rssKey\"] = \"rssList.json\""
      ],
      "metadata": {
        "id": "VbzRoqao8Z87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# CNBC Top News\n",
        "topNews = {\"network\":\"CNBC\", \"feed\":\"Top News\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114\"}\n",
        "# Earnings\n",
        "earning = {\"network\":\"CNBC\", \"feed\":\"Earnings\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839135\"}\n",
        "# Economy\n",
        "economy = {\"network\":\"CNBC\", \"feed\":\"Economy\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=20910258\"}\n",
        "# Finance\n",
        "finance = {\"network\":\"CNBC\", \"feed\":\"Finance\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664\"}\n",
        "# Tech\n",
        "tech = {\"network\":\"CNBC\", \"feed\":\"Tech\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910\"}\n",
        "# Investing\n",
        "invest = {\"network\":\"CNBC\", \"feed\":\"Investing\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069\"}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ilmY-YssHd1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ],
      "metadata": {
        "id": "-aHDFHqfNEeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "import boto3\n",
        "import datetime"
      ],
      "metadata": {
        "id": "N52GAxVRMm6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape RSS Feeds"
      ],
      "metadata": {
        "id": "OwjSigWoMx_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the articles from an RSS feed\n",
        "# Arg: source [string] *contains RSS feed link*\n",
        "# Returns: entries [list of dictionaries] *contains articles in RSS feed*\n",
        "def fetchRSS(source):\n",
        "  # fetch the RSS feed\n",
        "  RSS = feedparser.parse(source)\n",
        "  # extract the entries from the retrieved feed\n",
        "  entries = RSS.entries\n",
        "  return entries"
      ],
      "metadata": {
        "id": "OSvIKGOu43Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse HTML"
      ],
      "metadata": {
        "id": "ezJS6W4UNLK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape an article using the url\n",
        "# Arg: url [string] **contains url for the article**\n",
        "# Returns: soup [BeautifulSoup object]\n",
        "def scraper(url):\n",
        "  # scrape the data from the url using requests and BeautifulSoup\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "  return soup\n",
        "\n",
        "# Extract text from the scraped/parsed article\n",
        "# Arg: soup [BeautifulSoup object]\n",
        "# Returns: result [string] **contains the text of the article**\n",
        "def textExtract(soup):\n",
        "  # extract the data corresponding to the article from the soup\n",
        "  articleGroups = soup.find_all(\"div\", {\"class\": \"group\"})\n",
        "  sections = []\n",
        "  for group in articleGroups:\n",
        "    section = group.find_all(\"p\")\n",
        "    if section:\n",
        "      sections.append(section)\n",
        "  # extract the text of the article\n",
        "  text = []\n",
        "  for section in sections:\n",
        "    for para in section:\n",
        "      text.append(para.get_text())\n",
        "  # return the article joined as a single string\n",
        "  return (\"\\n\").join(text)"
      ],
      "metadata": {
        "id": "ktyhYTtANJhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# S3 Helper"
      ],
      "metadata": {
        "id": "iAXvRUXENIzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AccessS3:\n",
        "  def __init__(self):\n",
        "    session = boto3.Session()\n",
        "    self.s3 = session.client('s3')\n",
        "    self.paginator = self.s3.get_paginator('list_objects_v2')\n",
        "\n",
        "  # Get an object\n",
        "  # Arg: bucket **bucket name** [str],\n",
        "  #      key **object key** [str],\n",
        "  # Returns: object\n",
        "  def getObj(self, bucket, key):\n",
        "    return self.s3.get_object(Bucket=bucket, Key=key)\n",
        "\n",
        "  # Delete an object\n",
        "  # Arg: bucket **bucket name** [str],\n",
        "  #      key **object key** [str]\n",
        "  def deleteObj(self, bucket, key):\n",
        "    self.s3.delete_object(Bucket=bucket, Key=key)\n",
        "    print(\"Deleted object at {}\".format(key))\n",
        "    return 0\n",
        "\n",
        "  # Save an object\n",
        "  # Arg: data **data to be saved**\n",
        "  #      bucket **bucket name** [str],\n",
        "  #      key **object key** [str]\n",
        "  def saveObj(self, data, bucket, key):\n",
        "    self.s3.put_object(\n",
        "      Body=data,\n",
        "      Bucket=bucket,\n",
        "      Key=key\n",
        "    )\n",
        "    print(\"Saved object at {}\".format(key))\n",
        "    return 0\n",
        "\n",
        "  # Look at objects contained in a key\n",
        "  # Arg: bucket **bucket name** [str],\n",
        "  #      key **object key** [str]\n",
        "  # Returns: pages **objects in key**\n",
        "  def scanFolder(self, bucket, key):\n",
        "    pages = self.paginator.paginate(Bucket=bucket, Prefix=key)\n",
        "    return pages\n",
        "\n",
        "  # Look up an object\n",
        "  # Arg: bucket [string] **S3 bucket to look in**\n",
        "  #      folder [string] **folder to look in**\n",
        "  #      id [str] **lookup object id**\n",
        "  # Returns: key **object key if it exists**\n",
        "  def lookupObj(self, bucket, folder, query, group):\n",
        "    keys = []\n",
        "    pages = self.scanFolder(bucket, folder)\n",
        "    for page in pages:\n",
        "      for content in page['Contents']:\n",
        "        lookup = re.search(query, content[\"Key\"])\n",
        "        if lookup:\n",
        "          keys.append(lookup.group(group))\n",
        "    return keys"
      ],
      "metadata": {
        "id": "mrruG1N1r7wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save New Articles to S3"
      ],
      "metadata": {
        "id": "QUxoCxQb6OJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get publication dates of files\n",
        "# Arg: unstr_dates **unstructed dates** [list of str]\n",
        "# Returns: list of structured dates [list of str]\n",
        "def convertDates(unstr_dates):\n",
        "  s3Helper = AccessS3()\n",
        "  date_format = \"%a, %d %b %Y %H:%M:%S %Z\"\n",
        "  str_dates = []\n",
        "  for unstr_date in unstr_dates:\n",
        "    str_dates.append(datetime.datetime.strptime(unstr_date, date_format))\n",
        "  return str_dates"
      ],
      "metadata": {
        "id": "56VgP67lwL02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the text data and metadata for a new entry\n",
        "# Arg: newEntry [dict] **new article to be saved**\n",
        "#      network [string] **network that published the RSS feed**\n",
        "#      feed [string] **name of the RSS feed**\n",
        "#      bucket [string] **S3 bucket where data is stored**\n",
        "def saveData(newEntry, network, feed, bucket):\n",
        "  s3Helper = AccessS3()\n",
        "  # scrape and add the text to the entry\n",
        "  entryURL = newEntry[\"link\"]\n",
        "  soup = scraper(entryURL)\n",
        "  result = textExtract(soup)\n",
        "  newEntry[\"text\"] = result\n",
        "  # create object path\n",
        "  id = newEntry[\"id\"]\n",
        "  year = newEntry[\"published_parsed\"][0]\n",
        "  month = newEntry[\"published_parsed\"][1]\n",
        "  day = newEntry[\"published_parsed\"][2]\n",
        "  metakey = \"metadata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
        "  textkey = \"textdata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
        "  # collect metadata and text data\n",
        "  title = newEntry[\"title\"]\n",
        "  link = newEntry[\"link\"]\n",
        "  date = newEntry[\"published\"]\n",
        "  sponsor = newEntry[\"metadata_sponsored\"]\n",
        "  metatype = newEntry[\"metadata_type\"]\n",
        "  metadata = json.dumps({\"title\":title,\"link\":link,\"date\":date,\"feed\":feed,\"sponsor\":sponsor,\"type\":metatype})\n",
        "  textdata = json.dumps(newEntry[\"text\"])\n",
        "  # upload metadata\n",
        "  s3Helper.saveObj(metadata, bucket, metakey)\n",
        "  # upload text data\n",
        "  s3Helper.saveObj(textdata, bucket, textkey)\n",
        "  return metakey, textkey"
      ],
      "metadata": {
        "id": "9tt1okDhwIGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save new articles to S3 and update old ones\n",
        "# Arg: entries [list of dictionaries] **entries gathered from feed**\n",
        "#      network [string] **network that published the RSS feed**\n",
        "#      feed [string] **name of the RSS feed**\n",
        "#      bucket [string] **S3 bucket where data is stored**\n",
        "#      folder [string] **folder where data is stored**\n",
        "def saveNewEntries(entries, network, feed, bucket):\n",
        "  s3Helper = AccessS3()\n",
        "  count = 0\n",
        "  print(\"This feed has {} articles\".format(len(entries)))\n",
        "  for entry in entries:\n",
        "    # get the article id\n",
        "    id = entry[\"id\"]\n",
        "    # look up id to see if it exists already (i.e. has been saved before)\n",
        "    query = \".*\"+id+\"\\.[\\D]{4}$\"\n",
        "    group = 0\n",
        "    metakeys = s3Helper.lookupObj(bucket, \"metadata\", query, group)\n",
        "    textkeys = s3Helper.lookupObj(bucket, \"textdata\", query, group)\n",
        "    metadates = []\n",
        "    for metakey in metakeys:\n",
        "      metadata = (json.loads(s3Helper.getObj(bucket, metakey)['Body'].read().decode()))\n",
        "      metadates.append(metadata[\"date\"])\n",
        "\n",
        "    # if metakey is empty (i.e. it does not exist),\n",
        "    if not metakeys:\n",
        "      # then save it to s3\n",
        "      saveData(entry, network, feed, bucket)\n",
        "      count += 1\n",
        "    # otherwise it exists\n",
        "    else:\n",
        "      # convert dates to comparable datetime objects\n",
        "      existingDates = convertDates(metadates)\n",
        "      newestDate = max(existingDates)\n",
        "      entryDate = convertDates([entry[\"published\"]])[0]\n",
        "      # if the article has been updated,\n",
        "      if entryDate > newestDate:\n",
        "        for metakey, textkey in zip(metakeys, textkeys):\n",
        "          # delete the old one(s)\n",
        "          #print(\"Deleted {}\".format(metakey))\n",
        "          #print(\"Deleted {}\".format(textkey))\n",
        "          s3Helper.deleteObj(bucket, metakey)\n",
        "          s3Helper.deleteObj(bucket, textkey)\n",
        "        # and save the new one\n",
        "        entryMetaKey, entryTextKey = saveData(entry, network, feed, bucket)\n",
        "        count += 1\n",
        "      # otherwise check for existing duplicates\n",
        "      elif len(existingDates) > 1:\n",
        "        for metakey, textkey, existingDate in zip(metakeys, textkeys, existingDates):\n",
        "          if not existingDate==newestDate:\n",
        "            # and delete the older ones\n",
        "            #print(\"Deleted {}\".format(metakey))\n",
        "            #print(\"Deleted {}\".format(textkey))\n",
        "            s3Helper.deleteObj(bucket, metakey)\n",
        "            s3Helper.deleteObj(bucket, textkey)\n",
        "  # report the total number of articles that have been saved\n",
        "  if count==0:\n",
        "    print('No new entries for {}'.format(feed))\n",
        "  else:\n",
        "    print(\"{} new articles for {}\".format(count, feed))\n",
        "  return 0"
      ],
      "metadata": {
        "id": "F6_QRSNJLmok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "b2xBf8an5Pvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(event, context):\n",
        "  bucket = os.environ[\"bucket\"]\n",
        "  rssKey = os.environ[\"rssKey\"]\n",
        "  s3Helper = AccessS3()\n",
        "  # Retrieve list of RSS feeds to gather articles from\n",
        "  obj = s3Helper.getObj(bucket, rssKey)\n",
        "  rssList = json.loads(obj[\"Body\"].read().decode())\n",
        "  # Gather articles from the feeds and save new ones to S3\n",
        "  count = 0\n",
        "  for rss in rssList:\n",
        "    rssNetwork = rss[\"network\"]\n",
        "    rssFeed = rss[\"feed\"]\n",
        "    rssURL = rss[\"url\"]\n",
        "    # Fetch entries from the current RSS feed\n",
        "    print(\"Currently scraping from {}\".format(rssFeed))\n",
        "    entries = fetchRSS(rssURL)\n",
        "    # Save new entries to S3\n",
        "    saveNewEntries(entries, rssNetwork, rssFeed, bucket)\n",
        "  return {\n",
        "      'statusCode': 200\n",
        "  }"
      ],
      "metadata": {
        "id": "WnDjLKc35Qa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "VjSuiHoVI30z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "  # import text and test\n",
        "  result = main(None, None)\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "B6Oha02K5hjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f22c3993-867a-4e18-ac38-87aa57b2ea51",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently scraping from Top News\n",
            "This feed has 30 articles\n",
            "Deleted metadata/CNBC/2024/12/20/108078661.json\n",
            "Deleted textdata/CNBC/2024/12/20/108078661.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078661.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078661.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078787.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078787.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078701.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078701.json\n",
            "Deleted metadata/CNBC/2024/12/20/108078604.json\n",
            "Deleted textdata/CNBC/2024/12/20/108078604.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078604.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078604.json\n",
            "Deleted metadata/CNBC/2024/12/20/108078672.json\n",
            "Deleted textdata/CNBC/2024/12/20/108078672.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078672.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078672.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078717.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078717.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078778.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078778.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078267.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078267.json\n",
            "Deleted metadata/CNBC/2024/12/19/108078402.json\n",
            "Deleted textdata/CNBC/2024/12/19/108078402.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078782.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078782.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108077066.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108077066.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078530.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078530.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078709.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078709.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078341.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078341.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078769.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078769.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078692.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078692.json\n",
            "Deleted metadata/CNBC/2024/12/20/108078652.json\n",
            "Deleted textdata/CNBC/2024/12/20/108078652.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078652.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078652.json\n",
            "Deleted metadata/CNBC/2024/12/19/108078315.json\n",
            "Deleted textdata/CNBC/2024/12/19/108078315.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078315.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078315.json\n",
            "Deleted metadata/CNBC/2024/12/20/108078518.json\n",
            "Deleted textdata/CNBC/2024/12/20/108078518.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078518.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078518.json\n",
            "Deleted metadata/CNBC/2024/12/19/108077011.json\n",
            "Deleted textdata/CNBC/2024/12/19/108077011.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108076661.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108076661.json\n",
            "Deleted metadata/CNBC/2024/12/19/108078502.json\n",
            "Deleted textdata/CNBC/2024/12/19/108078502.json\n",
            "19 new articles for Top News\n",
            "Currently scraping from Earnings\n",
            "This feed has 30 articles\n",
            "Deleted metadata/CNBC/2024/12/19/108078502.json\n",
            "Deleted textdata/CNBC/2024/12/19/108078502.json\n",
            "Deleted metadata/CNBC/2024/12/19/108077011.json\n",
            "Deleted textdata/CNBC/2024/12/19/108077011.json\n",
            "No new entries for Earnings\n",
            "Currently scraping from Economy\n",
            "This feed has 30 articles\n",
            "No new entries for Economy\n",
            "Currently scraping from Finance\n",
            "This feed has 30 articles\n",
            "Saved object at metadata/CNBC/2024/12/20/108078705.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078705.json\n",
            "1 new articles for Finance\n",
            "Currently scraping from Tech\n",
            "This feed has 30 articles\n",
            "Deleted metadata/CNBC/2024/12/20/108078652.json\n",
            "Deleted textdata/CNBC/2024/12/20/108078652.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078652.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078652.json\n",
            "1 new articles for Tech\n",
            "Currently scraping from Investing\n",
            "This feed has 30 articles\n",
            "Saved object at metadata/CNBC/2024/12/20/108076726.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108076726.json\n",
            "Deleted metadata/CNBC/2024/12/20/108078685.json\n",
            "Deleted textdata/CNBC/2024/12/20/108078685.json\n",
            "Saved object at metadata/CNBC/2024/12/20/108078685.json\n",
            "Saved object at textdata/CNBC/2024/12/20/108078685.json\n",
            "2 new articles for Investing\n",
            "{'statusCode': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-CTageP_w18"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}