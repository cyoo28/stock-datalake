{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8vB3P4pvKMdSpa0eR9xZ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iyoo2018/findatalake/blob/master/scrapeRSS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Exclusive to Colab"
      ],
      "metadata": {
        "id": "uB_hhdZs40Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "  # Mount Google Drive to notebook\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  import sys\n",
        "  sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"
      ],
      "metadata": {
        "id": "XgNE46dr7JFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd583ca1-b221-4d5d-f41f-01c6872664b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "  # Set configuration file to access AWS\n",
        "  os.environ['AWS_CONFIG_FILE']=\"/content/gdrive/My Drive/cred-stockdata.txt\"\n",
        "  # Set environment variables\n",
        "  os.environ[\"bucket\"] = \"026090555438-stockdata\"\n",
        "  os.environ[\"rssKey\"] = \"rssList.json\""
      ],
      "metadata": {
        "id": "VbzRoqao8Z87"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# CNBC Top News\n",
        "topNews = {\"network\":\"CNBC\", \"feed\":\"Top News\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114\"}\n",
        "# Earnings\n",
        "earning = {\"network\":\"CNBC\", \"feed\":\"Earnings\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839135\"}\n",
        "# Economy\n",
        "economy = {\"network\":\"CNBC\", \"feed\":\"Economy\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=20910258\"}\n",
        "# Finance\n",
        "finance = {\"network\":\"CNBC\", \"feed\":\"Finance\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664\"}\n",
        "# Tech\n",
        "tech = {\"network\":\"CNBC\", \"feed\":\"Tech\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910\"}\n",
        "# Investing\n",
        "invest = {\"network\":\"CNBC\", \"feed\":\"Investing\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069\"}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ilmY-YssHd1H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "97d58779-aa16-4fb9-a334-a80cefc3a30f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# CNBC Top News\\ntopNews = {\"network\":\"CNBC\", \"feed\":\"Top News\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114\"}\\n# Earnings\\nearning = {\"network\":\"CNBC\", \"feed\":\"Earnings\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839135\"}\\n# Economy\\neconomy = {\"network\":\"CNBC\", \"feed\":\"Economy\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=20910258\"}\\n# Finance\\nfinance = {\"network\":\"CNBC\", \"feed\":\"Finance\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664\"}\\n# Tech\\ntech = {\"network\":\"CNBC\", \"feed\":\"Tech\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910\"}\\n# Investing\\ninvest = {\"network\":\"CNBC\", \"feed\":\"Investing\", \"url\":\"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069\"}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ],
      "metadata": {
        "id": "-aHDFHqfNEeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import boto3\n",
        "import datetime"
      ],
      "metadata": {
        "id": "N52GAxVRMm6N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AccessS3 Class"
      ],
      "metadata": {
        "id": "ppHfapzEzUBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for accessing s3\n",
        "class AccessS3:\n",
        "  def __init__(self):\n",
        "    session = boto3.Session()\n",
        "    self.s3 = session.client('s3')\n",
        "    self.paginator = self.s3.get_paginator('list_objects_v2')\n",
        "\n",
        "  # Get an object\n",
        "  # Arg: bucket [str] **bucket name**,\n",
        "  #      key [str] **object key**,\n",
        "  # Returns: [s3 obj]\n",
        "  def getObj(self, bucket, key):\n",
        "    return self.s3.get_object(Bucket=bucket, Key=key)\n",
        "\n",
        "  # Delete an object\n",
        "  # Arg: bucket [str] **bucket name**,\n",
        "  #      key [str] **object key**\n",
        "  def deleteObj(self, bucket, key):\n",
        "    self.s3.delete_object(Bucket=bucket, Key=key)\n",
        "    print(\"Deleted object at {}\".format(key))\n",
        "    return 0\n",
        "\n",
        "  # Save an object\n",
        "  # Arg: data [obj] **data to be saved**\n",
        "  #      bucket [str] **bucket name**,\n",
        "  #      key [str] **object key**\n",
        "  def saveObj(self, data, bucket, key):\n",
        "    self.s3.put_object(\n",
        "      Body=data,\n",
        "      Bucket=bucket,\n",
        "      Key=key\n",
        "    )\n",
        "    print(\"Saved object at {}\".format(key))\n",
        "    return 0\n",
        "\n",
        "  # Return objects contained in a key\n",
        "  # Arg: bucket [str] **bucket name**,\n",
        "  #      key [str] **object key**\n",
        "  # Returns: objs [list of s3 objs] **objects in key**\n",
        "  def scanFolder(self, bucket, key):\n",
        "    objs = []\n",
        "    # Get all object keys in bucket and key pair\n",
        "    pages = self.paginator.paginate(Bucket=bucket, Prefix=key)\n",
        "    # Convert iterator to list\n",
        "    for page in pages:\n",
        "      for content in page['Contents']:\n",
        "        # Only add object keys that do not end in \"/\" (folder keys end in \"/\")\n",
        "        if not content['Key'].endswith(\"/\"):\n",
        "          objs.append(content['Key'])\n",
        "    return objs\n",
        "\n",
        "  # Look up a specific object\n",
        "  # Arg: bucket [str] **bucket name**\n",
        "  #      key [str] **lookup key**\n",
        "  #      subKey [str] **substring of key to look for**\n",
        "  # Returns: matchObjs [list of s3 objs] **object key if it exists**\n",
        "  def lookupObj(self, bucket, key, subKey):\n",
        "    matchObjs = []\n",
        "    # Return objects contained in a key\n",
        "    objs = self.scanFolder(bucket, key)\n",
        "    # Add object keys that contain the lookup substring\n",
        "    for obj in objs:\n",
        "      if subKey in obj:\n",
        "        matchObjs.append(obj)\n",
        "    return matchObjs"
      ],
      "metadata": {
        "id": "c-1TvTw1zT5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SaveStockData Class"
      ],
      "metadata": {
        "id": "D7RoyHvAzUh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for saving stock data to s3\n",
        "class SaveStockData:\n",
        "  def __init__(self):\n",
        "    session = boto3.Session()\n",
        "    self.s3 = session.client('s3')\n",
        "\n",
        "  # Create key path for metadata\n",
        "  # Arg: newEntry [dict] **entry from RSS feed**,\n",
        "  #      network [str] **network that published the RSS feed**\n",
        "  # Returns: metaKey [str] **meta key**\n",
        "  def createMetaPath(self, newEntry, network):\n",
        "    # Gather necessary info from entry to create key\n",
        "    id = newEntry[\"id\"]\n",
        "    year = newEntry[\"published_parsed\"][0]\n",
        "    month = newEntry[\"published_parsed\"][1]\n",
        "    day = newEntry[\"published_parsed\"][2]\n",
        "    # Create key\n",
        "    metaKey = \"metadata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
        "    return metaKey\n",
        "\n",
        "  # Create key path for textdata\n",
        "  # Arg: newEntry [dict] **entry from RSS feed**,\n",
        "  #      network [str] **network that published the RSS feed**\n",
        "  # Returns: textKey [str] **text key**\n",
        "  def createTextPath(self, newEntry, network):\n",
        "    # Gather necessary info from entry to create key\n",
        "    id = newEntry[\"id\"]\n",
        "    year = newEntry[\"published_parsed\"][0]\n",
        "    month = newEntry[\"published_parsed\"][1]\n",
        "    day = newEntry[\"published_parsed\"][2]\n",
        "    # Create key\n",
        "    textKey = \"textdata/{}/{}/{}/{}/{}.json\".format(network, year, month, day, id)\n",
        "    return textKey\n",
        "\n",
        "  # Collect metadata to save\n",
        "  # Arg: newEntry [dict] **entry from RSS feed**,\n",
        "  #      feed [str] **RSS feed name**\n",
        "  # Returns: metaData [json dict] **meta data**\n",
        "  def collectMetaData(self, newEntry, feed):\n",
        "    title = newEntry[\"title\"]\n",
        "    link = newEntry[\"link\"]\n",
        "    date = newEntry[\"published\"]\n",
        "    sponsor = newEntry[\"metadata_sponsored\"]\n",
        "    metaType = newEntry[\"metadata_type\"]\n",
        "    metaData = json.dumps({\"title\":title,\"link\":link,\"date\":date,\"feed\":feed,\"sponsor\":sponsor,\"type\":metaType})\n",
        "    return metaData\n",
        "\n",
        "  # Collect textdata to save\n",
        "  # Arg: newEntry [dict] **entry from RSS feed**,\n",
        "  #      feed [str] **RSS feed name**\n",
        "  # Returns: textData [json str] **text data**\n",
        "  def collectTextData(self, newEntry):\n",
        "    textData = json.dumps(newEntry[\"text\"])\n",
        "    return textData\n",
        "\n",
        "  # Save data to s3\n",
        "  # Arg: data [obj] **data to be saved**,\n",
        "  #      bucket [str] **bucket name**,\n",
        "  #      key [str] **key to save to**\n",
        "  def saveData(self, data, bucket, key):\n",
        "    self.s3.put_object(\n",
        "        Body=data,\n",
        "        Bucket=bucket,\n",
        "        Key=key\n",
        "    )\n",
        "    return 0"
      ],
      "metadata": {
        "id": "CQI-ts6TzURt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape RSS Feeds"
      ],
      "metadata": {
        "id": "OwjSigWoMx_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the entries from an RSS feed\n",
        "# Arg: source [str] **contains RSS feed link**\n",
        "# Returns: entries [list of dict] **contains entries in RSS feed**\n",
        "def fetchRSS(source):\n",
        "  # fetch the RSS feed\n",
        "  RSS = feedparser.parse(source)\n",
        "  # extract the entries from the retrieved feed\n",
        "  entries = RSS.entries\n",
        "  return entries"
      ],
      "metadata": {
        "id": "OSvIKGOu43Cc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse HTML"
      ],
      "metadata": {
        "id": "ezJS6W4UNLK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape an entry using the url\n",
        "# Arg: url [str] **contains url for the entry**\n",
        "# Returns: soup [BeautifulSoup obj]\n",
        "def scraper(url):\n",
        "  # scrape the data from the url using requests and BeautifulSoup\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "  return soup\n",
        "\n",
        "# Extract text from the scraped/parsed entry\n",
        "# Arg: soup [BeautifulSoup obj]\n",
        "# Returns: result [str] **contains the text of the entry**\n",
        "def textExtract(soup):\n",
        "  # extract the data corresponding to the entry from the soup\n",
        "  entryGroups = soup.find_all(\"div\", {\"class\": \"group\"})\n",
        "  sections = []\n",
        "  for group in entryGroups:\n",
        "    section = group.find_all(\"p\")\n",
        "    if section:\n",
        "      sections.append(section)\n",
        "  # extract the text of the entry\n",
        "  text = []\n",
        "  for section in sections:\n",
        "    for para in section:\n",
        "      text.append(para.get_text())\n",
        "  # return the entry joined as a single string\n",
        "  return (\"\\n\").join(text)"
      ],
      "metadata": {
        "id": "ktyhYTtANJhh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get publication dates"
      ],
      "metadata": {
        "id": "jn0wyZQGrT2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get publication dates of files\n",
        "# Arg: bucket [str] **bucket name**,\n",
        "#      keys [list of str] **list of keys that require dates**\n",
        "#      s3Helper [AccessS3 inst]\n",
        "# Returns: dates [list of str] **list of dates for keys**\n",
        "def getDates(bucket, keys, s3Helper):\n",
        "  dates = []\n",
        "  for key in keys:\n",
        "    date = (json.loads(s3Helper.getObj(bucket, key)['Body'].read().decode()))\n",
        "    dates.append(date[\"date\"])\n",
        "  return dates"
      ],
      "metadata": {
        "id": "aps_rImQrTqK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert publication dates"
      ],
      "metadata": {
        "id": "6Ei3HbWBPlZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert publication dates of files\n",
        "# Arg: unstrDates [list of str] **unstructed dates**\n",
        "# Returns: [list of str] **list of structured dates**\n",
        "def convertDates(unstrDates):\n",
        "  dateFormat = \"%a, %d %b %Y %H:%M:%S %Z\"\n",
        "  return [datetime.datetime.strptime(unstrDate, dateFormat) for unstrDate in unstrDates]"
      ],
      "metadata": {
        "id": "SrBZJVB8PlMV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save New Entries to S3"
      ],
      "metadata": {
        "id": "QUxoCxQb6OJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the text data and metadata for a new entry\n",
        "# Arg: newEntry [dict] **new entry to be saved**,\n",
        "#      network [str] **network that published the RSS feed**,\n",
        "#      feed [str] **name of the RSS feed**,\n",
        "#      bucket [str] **S3 bucket where data is stored**\n",
        "def saveSingleEntry(newEntry, network, feed, bucket):\n",
        "  stockSaver = SaveStockData()\n",
        "\n",
        "  # scrape and add the text to the entry\n",
        "  entryURL = newEntry[\"link\"]\n",
        "  soup = scraper(entryURL)\n",
        "  result = textExtract(soup)\n",
        "  newEntry[\"text\"] = result\n",
        "\n",
        "  # create object path\n",
        "  metaKey = stockSaver.createMetaPath(newEntry, network)\n",
        "  textKey = stockSaver.createTextPath(newEntry, network)\n",
        "\n",
        "  # collect metadata and text data\n",
        "  metaData = stockSaver.collectMetaData(newEntry, feed)\n",
        "  textData = stockSaver.collectTextData(newEntry)\n",
        "\n",
        "  # upload metadata\n",
        "  stockSaver.saveData(metaData, bucket, metaKey)\n",
        "  # upload text data\n",
        "  stockSaver.saveData(textData, bucket, textKey)\n",
        "\n",
        "  return 0"
      ],
      "metadata": {
        "id": "9tt1okDhwIGA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save new entries to S3 and update old ones\n",
        "# Arg: entries [list of dict] **entries gathered from feed**\n",
        "#      network [str] **network that published the RSS feed**\n",
        "#      feed [str] **name of the RSS feed**\n",
        "#      bucket [str] **bucket where data is stored**\n",
        "#      s3Helper [AccessS3 inst]\n",
        "def saveNewEntries(entries, network, feed, bucket, s3Helper):\n",
        "  # Setup save counter\n",
        "  count = 0\n",
        "  print(\"This feed has {} entries\".format(len(entries)))\n",
        "  # Get the existing entries from s3\n",
        "  existKeys = s3Helper.scanFolder(bucket, \"metadata\")\n",
        "  for entry in entries:\n",
        "    # Get the entry id\n",
        "    entryID = entry[\"id\"]\n",
        "    # if the entry id does not already exist,\n",
        "    if not any(entryID in existKey for existKey in existKeys):\n",
        "      # then save it to s3\n",
        "      print(\"Saving new entry id {}:\".format(entryID))\n",
        "      saveSingleEntry(entry, network, feed, bucket)\n",
        "      count += 1\n",
        "    # otherwise it already exists\n",
        "    else:\n",
        "      # Search for matching entries\n",
        "      matchKeys = [existKey.split(\"/\", 1)[1] for existKey in existKeys if entryID in existKey]\n",
        "      metaKeys = [\"metadata/\"+matchKey for matchKey in matchKeys]\n",
        "      textKeys = [\"textdata/\"+matchKey for matchKey in matchKeys]\n",
        "      # Retrieve and convert dates to comparable datetime objects\n",
        "      matchDates = getDates(bucket, metaKeys, s3Helper)\n",
        "      existDates = convertDates(matchDates)\n",
        "      latestDate = max(existDates)\n",
        "      entryDate = convertDates([entry[\"published\"]])[0]\n",
        "      # if the entry has been updated,\n",
        "      if entryDate > latestDate:\n",
        "        print(\"Updating entry id: {}\".format(entryID))\n",
        "        for metaKey, textKey in zip(metaKeys, textKeys):\n",
        "          # delete the old one(s)\n",
        "          #print(\"Deleted {}\".format(metaKey))\n",
        "          #print(\"Deleted {}\".format(textKey))\n",
        "          s3Helper.deleteObj(bucket, metaKey)\n",
        "          s3Helper.deleteObj(bucket, textKey)\n",
        "        # and save the new one\n",
        "        saveSingleEntry(entry, network, feed, bucket)\n",
        "        count += 1\n",
        "      # otherwise check for duplicates\n",
        "      elif len(matchKeys) > 1:\n",
        "        print(\"Deleting duplicate entries id: {}\".format(entryID))\n",
        "        for metaKey, textKey, existDate in zip(metaKeys, textKeys, existDates):\n",
        "          if not existDate==latestDate:\n",
        "            # and delete the older ones\n",
        "            #print(\"Deleted {}\".format(metaKey))\n",
        "            #print(\"Deleted {}\".format(textKey))\n",
        "            s3Helper.deleteObj(bucket, metaKey)\n",
        "            s3Helper.deleteObj(bucket, textKey)\n",
        "  # Report the total number of entries that have been saved\n",
        "  if count==0:\n",
        "    print('No new entries for {}'.format(feed))\n",
        "  else:\n",
        "    print(\"{} new entries for {}\".format(count, feed))\n",
        "  return 0"
      ],
      "metadata": {
        "id": "F6_QRSNJLmok"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "b2xBf8an5Pvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(event, context):\n",
        "  # Set up variables and AccessS3 class\n",
        "  bucket = os.environ[\"bucket\"]\n",
        "  rssKey = os.environ[\"rssKey\"]\n",
        "  s3Helper = AccessS3()\n",
        "  # Retrieve list of RSS feeds to gather entries from\n",
        "  obj = s3Helper.getObj(bucket, rssKey)\n",
        "  rssList = json.loads(obj[\"Body\"].read().decode())\n",
        "  # Gather entries from the feeds and save new ones to S3\n",
        "  count = 0\n",
        "  for rss in rssList:\n",
        "    rssNetwork = rss[\"network\"]\n",
        "    rssFeed = rss[\"feed\"]\n",
        "    rssURL = rss[\"url\"]\n",
        "    # Fetch entries from the current RSS feed\n",
        "    print(\"Currently scraping from {}\".format(rssFeed))\n",
        "    entries = fetchRSS(rssURL)\n",
        "    # Save new entries to S3\n",
        "    saveNewEntries(entries, rssNetwork, rssFeed, bucket, s3Helper)\n",
        "  return {\n",
        "      'statusCode': 200\n",
        "  }"
      ],
      "metadata": {
        "id": "WnDjLKc35Qa4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "VjSuiHoVI30z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "  # import text and test\n",
        "  result = main(None, None)\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "B6Oha02K5hjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca86380f-d7e1-4105-f765-9bf7cc027f1e",
        "collapsed": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently scraping from Top News\n",
            "This feed has 30 entries\n",
            "No new entries for Top News\n",
            "Currently scraping from Earnings\n",
            "This feed has 30 entries\n",
            "No new entries for Earnings\n",
            "Currently scraping from Economy\n",
            "This feed has 30 entries\n",
            "No new entries for Economy\n",
            "Currently scraping from Finance\n",
            "This feed has 30 entries\n",
            "No new entries for Finance\n",
            "Currently scraping from Tech\n",
            "This feed has 30 entries\n",
            "No new entries for Tech\n",
            "Currently scraping from Investing\n",
            "This feed has 30 entries\n",
            "No new entries for Investing\n",
            "{'statusCode': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-CTageP_w18"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}